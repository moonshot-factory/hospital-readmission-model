{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hospital Readmission Risk Model - Model Training\n",
        "\n",
        "**Project:** Hospital Readmission Risk Prediction  \n",
        "**Timeline:** January 2015 - May 2015  \n",
        "**Author:** Blake [Your Last Name]  \n",
        "\n",
        "## Objective\n",
        "Train and optimize machine learning models for 30-day readmission prediction:\n",
        "- Logistic Regression for interpretability\n",
        "- Decision Trees for rule-based insights\n",
        "- Cross-validation for robust evaluation\n",
        "- Hyperparameter optimization\n",
        "- Clinical relevance validation\n",
        "\n",
        "**Focus**: Model interpretability was crucial for clinical adoption. Healthcare professionals needed to understand and trust the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Model training libraries imported successfully\")\n",
        "print(f\"Training session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n",
        "\n",
        "Starting with the clean, feature-engineered dataset from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the processed dataset (in practice, this would be loaded from the feature engineering notebook)\n",
        "def create_processed_dataset():\n",
        "    \"\"\"\n",
        "    Recreate the processed dataset for model training\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_patients = 5000\n",
        "    \n",
        "    # Core features\n",
        "    age = np.random.normal(65, 15, n_patients)\n",
        "    age = np.clip(age, 18, 100)\n",
        "    \n",
        "    length_of_stay = np.random.exponential(4, n_patients)\n",
        "    length_of_stay = np.clip(length_of_stay, 1, 20)  # Outliers clipped\n",
        "    \n",
        "    previous_admissions = np.random.poisson(1.5, n_patients)\n",
        "    emergency_admission = np.random.binomial(1, 0.6, n_patients)\n",
        "    \n",
        "    # Medical conditions (engineered from codes)\n",
        "    has_diabetes = np.random.binomial(1, 0.3, n_patients)\n",
        "    has_hypertension = np.random.binomial(1, 0.4, n_patients)\n",
        "    has_heart_disease = np.random.binomial(1, 0.25, n_patients)\n",
        "    has_kidney_disease = np.random.binomial(1, 0.15, n_patients)\n",
        "    has_hyperlipidemia = np.random.binomial(1, 0.35, n_patients)\n",
        "    \n",
        "    # Derived features\n",
        "    comorbidity_count = has_diabetes + has_hypertension + has_heart_disease + has_kidney_disease + has_hyperlipidemia\n",
        "    high_risk_patient = ((age >= 75) | (previous_admissions >= 3) | (comorbidity_count >= 3)).astype(int)\n",
        "    emergency_elderly = (emergency_admission & (age >= 65)).astype(int)\n",
        "    \n",
        "    # Temporal features\n",
        "    days_since_last_admission = np.where(\n",
        "        previous_admissions > 0,\n",
        "        np.random.exponential(60, n_patients),\n",
        "        999\n",
        "    )\n",
        "    recent_admission = (days_since_last_admission <= 30).astype(int)\n",
        "    frequent_readmitter = ((previous_admissions >= 2) & (days_since_last_admission <= 90)).astype(int)\n",
        "    \n",
        "    # Categorical features (one-hot encoded)\n",
        "    gender_std_Female = np.random.binomial(1, 0.52, n_patients)\n",
        "    gender_std_Male = 1 - gender_std_Female\n",
        "    \n",
        "    insurance_std_Medicare = np.random.binomial(1, 0.45, n_patients)\n",
        "    insurance_std_Private = np.where(insurance_std_Medicare == 0, np.random.binomial(1, 0.6, n_patients), 0)\n",
        "    insurance_std_Medicaid = np.where((insurance_std_Medicare == 0) & (insurance_std_Private == 0), \n",
        "                                     np.random.binomial(1, 0.7, n_patients), 0)\n",
        "    insurance_std_Other = 1 - insurance_std_Medicare - insurance_std_Private - insurance_std_Medicaid\n",
        "    \n",
        "    # Age groups\n",
        "    age_group_Under_40 = (age < 40).astype(int)\n",
        "    age_group_40_60 = ((age >= 40) & (age < 60)).astype(int)\n",
        "    age_group_60_80 = ((age >= 60) & (age < 80)).astype(int)\n",
        "    age_group_Over_80 = (age >= 80).astype(int)\n",
        "    \n",
        "    # Create realistic readmission probabilities\n",
        "    readmission_prob = (\n",
        "        0.05 +  # baseline\n",
        "        0.004 * (age - 50) +  # age effect\n",
        "        0.15 * has_diabetes +\n",
        "        0.1 * has_hypertension +\n",
        "        0.2 * has_heart_disease +\n",
        "        0.25 * has_kidney_disease +\n",
        "        0.05 * has_hyperlipidemia +\n",
        "        0.02 * length_of_stay +\n",
        "        0.08 * previous_admissions +\n",
        "        0.12 * emergency_admission +\n",
        "        0.3 * recent_admission +\n",
        "        0.4 * frequent_readmitter +\n",
        "        np.random.normal(0, 0.05, n_patients)\n",
        "    )\n",
        "    \n",
        "    readmission_prob = np.clip(readmission_prob, 0, 1)\n",
        "    readmission_30_day = np.random.binomial(1, readmission_prob, n_patients)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'age': age,\n",
        "        'length_of_stay': length_of_stay,\n",
        "        'previous_admissions': previous_admissions,\n",
        "        'emergency_admission': emergency_admission,\n",
        "        'has_diabetes': has_diabetes,\n",
        "        'has_hypertension': has_hypertension,\n",
        "        'has_heart_disease': has_heart_disease,\n",
        "        'has_kidney_disease': has_kidney_disease,\n",
        "        'has_hyperlipidemia': has_hyperlipidemia,\n",
        "        'comorbidity_count': comorbidity_count,\n",
        "        'high_risk_patient': high_risk_patient,\n",
        "        'emergency_elderly': emergency_elderly,\n",
        "        'days_since_last_admission': days_since_last_admission,\n",
        "        'recent_admission': recent_admission,\n",
        "        'frequent_readmitter': frequent_readmitter,\n",
        "        'gender_std_Female': gender_std_Female,\n",
        "        'gender_std_Male': gender_std_Male,\n",
        "        'insurance_std_Medicare': insurance_std_Medicare,\n",
        "        'insurance_std_Private': insurance_std_Private,\n",
        "        'insurance_std_Medicaid': insurance_std_Medicaid,\n",
        "        'insurance_std_Other': insurance_std_Other,\n",
        "        'age_group_Under_40': age_group_Under_40,\n",
        "        'age_group_40_60': age_group_40_60,\n",
        "        'age_group_60_80': age_group_60_80,\n",
        "        'age_group_Over_80': age_group_Over_80,\n",
        "        'readmission_30_day': readmission_30_day\n",
        "    })\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Load the processed data\n",
        "df = create_processed_dataset()\n",
        "X = df.drop(['readmission_30_day'], axis=1)\n",
        "y = df['readmission_30_day']\n",
        "\n",
        "print(f\"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Positive class rate: {y.mean()*100:.1f}%\")\n",
        "print(f\"Feature data types: {X.dtypes.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Splitting and Preprocessing\n",
        "\n",
        "### Split data and prepare for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training positive rate: {y_train.mean()*100:.1f}%\")\n",
        "print(f\"Test positive rate: {y_test.mean()*100:.1f}%\")\n",
        "\n",
        "# Scale features for Logistic Regression\n",
        "# Note: Decision Trees don't require scaling, but LR does for optimal performance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(f\"\\nFeature scaling completed\")\n",
        "print(f\"Feature means after scaling: {X_train_scaled_df.mean().abs().max():.6f}\")\n",
        "print(f\"Feature stds after scaling: {X_train_scaled_df.std().max():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training - Logistic Regression\n",
        "\n",
        "### Primary focus on interpretability for clinical adoption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline Logistic Regression\n",
        "print(\"=== LOGISTIC REGRESSION TRAINING ===\")\n",
        "\n",
        "# Baseline model\n",
        "lr_baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_baseline.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Baseline predictions\n",
        "y_train_pred_lr = lr_baseline.predict(X_train_scaled)\n",
        "y_test_pred_lr = lr_baseline.predict(X_test_scaled)\n",
        "y_train_proba_lr = lr_baseline.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_lr = lr_baseline.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Baseline performance\n",
        "print(\"\\nBaseline Logistic Regression Results:\")\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_lr):.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_lr):.3f}\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_lr):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr):.3f}\")\n",
        "\n",
        "# Cross-validation for robust evaluation\n",
        "print(\"\\nCross-Validation Results:\")\n",
        "cv_scores = cross_val_score(lr_baseline, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "print(f\"CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "cv_scores_acc = cross_val_score(lr_baseline, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"CV Accuracy: {cv_scores_acc.mean():.3f} (+/- {cv_scores_acc.std() * 2:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Logistic Regression\n",
        "print(\"\\n=== LOGISTIC REGRESSION HYPERPARAMETER TUNING ===\")\n",
        "\n",
        "# Define parameter grid\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # Supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "lr_grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=1000),\n",
        "    lr_param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"Best parameters: {lr_grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {lr_grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Train final optimized model\n",
        "lr_optimized = lr_grid_search.best_estimator_\n",
        "y_train_pred_lr_opt = lr_optimized.predict(X_train_scaled)\n",
        "y_test_pred_lr_opt = lr_optimized.predict(X_test_scaled)\n",
        "y_train_proba_lr_opt = lr_optimized.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_lr_opt = lr_optimized.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nOptimized Logistic Regression Results:\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_lr_opt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr_opt):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr_opt):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr_opt):.3f}\")\n",
        "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_lr_opt):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training - Decision Tree\n",
        "\n",
        "### Decision trees for rule-based clinical insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Decision Tree\n",
        "print(\"=== DECISION TREE TRAINING ===\")\n",
        "\n",
        "# Baseline Decision Tree\n",
        "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
        "dt_baseline.fit(X_train, y_train)  # No scaling needed for trees\n",
        "\n",
        "# Baseline predictions\n",
        "y_train_pred_dt = dt_baseline.predict(X_train)\n",
        "y_test_pred_dt = dt_baseline.predict(X_test)\n",
        "y_train_proba_dt = dt_baseline.predict_proba(X_train)[:, 1]\n",
        "y_test_proba_dt = dt_baseline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nBaseline Decision Tree Results:\")\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_dt):.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_dt):.3f}\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_dt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_dt):.3f}\")\n",
        "print(f\"Tree depth: {dt_baseline.get_depth()}\")\n",
        "print(f\"Number of leaves: {dt_baseline.get_n_leaves()}\")\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores_dt = cross_val_score(dt_baseline, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "print(f\"CV ROC-AUC: {cv_scores_dt.mean():.3f} (+/- {cv_scores_dt.std() * 2:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Decision Tree\n",
        "print(\"\\n=== DECISION TREE HYPERPARAMETER TUNING ===\")\n",
        "\n",
        "# Parameter grid focused on preventing overfitting\n",
        "dt_param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "dt_grid_search = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    dt_param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dt_grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {dt_grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {dt_grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Train optimized Decision Tree\n",
        "dt_optimized = dt_grid_search.best_estimator_\n",
        "y_train_pred_dt_opt = dt_optimized.predict(X_train)\n",
        "y_test_pred_dt_opt = dt_optimized.predict(X_test)\n",
        "y_train_proba_dt_opt = dt_optimized.predict_proba(X_train)[:, 1]\n",
        "y_test_proba_dt_opt = dt_optimized.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nOptimized Decision Tree Results:\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_dt_opt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_dt_opt):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_dt_opt):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_dt_opt):.3f}\")\n",
        "print(f\"Tree depth: {dt_optimized.get_depth()}\")\n",
        "print(f\"Number of leaves: {dt_optimized.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance Analysis\n",
        "\n",
        "### Understanding which features drive predictions - crucial for clinical interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance from Logistic Regression (coefficients)\n",
        "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "\n",
        "# Logistic Regression coefficients\n",
        "lr_coefficients = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Coefficient': lr_optimized.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(lr_optimized.coef_[0])\n",
        "})\n",
        "lr_coefficients = lr_coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Logistic Regression Features (by coefficient magnitude):\")\n",
        "for _, row in lr_coefficients.head(10).iterrows():\n",
        "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
        "    print(f\"{row['Feature']}: {row['Coefficient']:.3f} ({direction} risk)\")\n",
        "\n",
        "# Decision Tree feature importance\n",
        "dt_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': dt_optimized.feature_importances_\n",
        "})\n",
        "dt_importance = dt_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Decision Tree Features (by importance):\")\n",
        "for _, row in dt_importance.head(10).iterrows():\n",
        "    print(f\"{row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Logistic Regression coefficients\n",
        "plt.subplot(2, 2, 1)\n",
        "top_lr_features = lr_coefficients.head(10)\n",
        "colors = ['red' if coef < 0 else 'blue' for coef in top_lr_features['Coefficient']]\n",
        "plt.barh(range(len(top_lr_features)), top_lr_features['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_lr_features)), top_lr_features['Feature'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Logistic Regression Feature Coefficients')\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Decision Tree importance\n",
        "plt.subplot(2, 2, 2)\n",
        "top_dt_features = dt_importance.head(10)\n",
        "plt.barh(range(len(top_dt_features)), top_dt_features['Importance'], alpha=0.7)\n",
        "plt.yticks(range(len(top_dt_features)), top_dt_features['Feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Decision Tree Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Feature comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "# Normalize coefficients to 0-1 scale for comparison\n",
        "lr_norm = lr_coefficients.set_index('Feature')['Abs_Coefficient']\n",
        "lr_norm = lr_norm / lr_norm.max()\n",
        "dt_norm = dt_importance.set_index('Feature')['Importance']\n",
        "dt_norm = dt_norm / dt_norm.max()\n",
        "\n",
        "# Get common top features\n",
        "common_features = list(set(lr_coefficients.head(8)['Feature']) & set(dt_importance.head(8)['Feature']))\n",
        "common_features = common_features[:6]  # Show top 6 common features\n",
        "\n",
        "x = np.arange(len(common_features))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, [lr_norm[feat] for feat in common_features], width, label='Logistic Regression', alpha=0.7)\n",
        "plt.bar(x + width/2, [dt_norm[feat] for feat in common_features], width, label='Decision Tree', alpha=0.7)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Normalized Importance')\n",
        "plt.title('Feature Importance Comparison')\n",
        "plt.xticks(x, [feat.replace('has_', '').replace('_', ' ') for feat in common_features], rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Clinical interpretation categories\n",
        "plt.subplot(2, 2, 4)\n",
        "clinical_categories = {\n",
        "    'Demographics': ['age', 'gender_std_Female', 'gender_std_Male'],\n",
        "    'Medical History': ['has_diabetes', 'has_hypertension', 'has_heart_disease', 'has_kidney_disease'],\n",
        "    'Clinical Metrics': ['length_of_stay', 'previous_admissions', 'emergency_admission'],\n",
        "    'Risk Factors': ['high_risk_patient', 'frequent_readmitter', 'recent_admission']\n",
        "}\n",
        "\n",
        "category_importance = {}\n",
        "for category, features in clinical_categories.items():\n",
        "    # Sum importance for features in each category\n",
        "    importance_sum = sum([dt_importance[dt_importance['Feature'] == feat]['Importance'].values[0] \n",
        "                         for feat in features if feat in dt_importance['Feature'].values])\n",
        "    category_importance[category] = importance_sum\n",
        "\n",
        "categories = list(category_importance.keys())\n",
        "importances = list(category_importance.values())\n",
        "plt.pie(importances, labels=categories, autopct='%1.1f%%')\n",
        "plt.title('Feature Importance by Clinical Category')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison and Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "print(\"=== MODEL PERFORMANCE COMPARISON ===\")\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression (Baseline)': (y_test_pred_lr, y_test_proba_lr),\n",
        "    'Logistic Regression (Optimized)': (y_test_pred_lr_opt, y_test_proba_lr_opt),\n",
        "    'Decision Tree (Baseline)': (y_test_pred_dt, y_test_proba_dt),\n",
        "    'Decision Tree (Optimized)': (y_test_pred_dt_opt, y_test_proba_dt_opt)\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC'])\n",
        "\n",
        "for i, (model_name, (y_pred, y_proba)) in enumerate(models.items()):\n",
        "    results_df.loc[i] = [\n",
        "        model_name,\n",
        "        accuracy_score(y_test, y_pred),\n",
        "        precision_score(y_test, y_pred),\n",
        "        recall_score(y_test, y_pred),\n",
        "        f1_score(y_test, y_pred),\n",
        "        roc_auc_score(y_test, y_proba)\n",
        "    ]\n",
        "\n",
        "# Format results for display\n",
        "for col in ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']:\n",
        "    results_df[col] = results_df[col].astype(float).round(3)\n",
        "\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Identify best models\n",
        "best_auc_model = results_df.loc[results_df['ROC-AUC'].idxmax(), 'Model']\n",
        "best_f1_model = results_df.loc[results_df['F1'].idxmax(), 'Model']\n",
        "print(f\"\\nBest ROC-AUC: {best_auc_model}\")\n",
        "print(f\"Best F1-Score: {best_f1_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# ROC Curves\n",
        "plt.subplot(2, 3, 1)\n",
        "for model_name, (_, y_proba) in models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend()\n",
        "\n",
        "# Precision-Recall Curves\n",
        "plt.subplot(2, 3, 2)\n",
        "for model_name, (_, y_proba) in models.items():\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    plt.plot(recall, precision, label=model_name)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend()\n",
        "\n",
        "# Performance metrics comparison\n",
        "plt.subplot(2, 3, 3)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.2\n",
        "\n",
        "for i, (model_name, _) in enumerate(models.items()):\n",
        "    model_scores = results_df[results_df['Model'] == model_name][metrics].values[0]\n",
        "    plt.bar(x + i*width, model_scores, width, label=model_name.split('(')[0].strip(), alpha=0.7)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics Comparison')\n",
        "plt.xticks(x + width*1.5, metrics)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Confusion matrices for best models\n",
        "plt.subplot(2, 3, 4)\n",
        "cm_lr = confusion_matrix(y_test, y_test_pred_lr_opt)\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "cm_dt = confusion_matrix(y_test, y_test_pred_dt_opt)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens',\n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Learning curves for overfitting analysis\n",
        "plt.subplot(2, 3, 6)\n",
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "train_scores_lr = []\n",
        "val_scores_lr = []\n",
        "\n",
        "for train_size in train_sizes:\n",
        "    n_samples = int(train_size * len(X_train_scaled))\n",
        "    X_subset = X_train_scaled[:n_samples]\n",
        "    y_subset = y_train[:n_samples]\n",
        "    \n",
        "    lr_temp = LogisticRegression(**lr_optimized.get_params())\n",
        "    lr_temp.fit(X_subset, y_subset)\n",
        "    \n",
        "    train_score = roc_auc_score(y_subset, lr_temp.predict_proba(X_subset)[:, 1])\n",
        "    val_score = roc_auc_score(y_test, lr_temp.predict_proba(X_test_scaled)[:, 1])\n",
        "    \n",
        "    train_scores_lr.append(train_score)\n",
        "    val_scores_lr.append(val_score)\n",
        "\n",
        "plt.plot(train_sizes, train_scores_lr, 'o-', label='Training Score', alpha=0.7)\n",
        "plt.plot(train_sizes, val_scores_lr, 'o-', label='Validation Score', alpha=0.7)\n",
        "plt.xlabel('Training Set Size Fraction')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.title('Learning Curve - Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Clinical Interpretation and Model Selection\n",
        "\n",
        "### Selecting the optimal model for clinical deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clinical model evaluation\n",
        "print(\"=== CLINICAL MODEL EVALUATION ===\")\n",
        "\n",
        "# Define clinical evaluation criteria\n",
        "def clinical_evaluation(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model from clinical perspective\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    \n",
        "    # Clinical-specific metrics\n",
        "    \n",
        "    # Sensitivity (recall) - ability to identify patients who will be readmitted\n",
        "    sensitivity = recall\n",
        "    \n",
        "    # Specificity - ability to correctly identify patients who won't be readmitted\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    \n",
        "    # Positive Predictive Value (precision) - of patients flagged as high-risk, how many actually are?\n",
        "    ppv = precision\n",
        "    \n",
        "    # Negative Predictive Value - of patients flagged as low-risk, how many actually are?\n",
        "    npv = tn / (tn + fn)\n",
        "    \n",
        "    # Number Needed to Screen - for every true positive, how many need intervention?\n",
        "    nns = 1 / precision if precision > 0 else float('inf')\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'AUC': auc,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'PPV': ppv,\n",
        "        'NPV': npv,\n",
        "        'NNS': nns\n",
        "    }\n",
        "\n",
        "# Evaluate all models clinically\n",
        "clinical_results = []\n",
        "for model_name, (y_pred, y_proba) in models.items():\n",
        "    result = clinical_evaluation(y_test, y_pred, y_proba, model_name)\n",
        "    clinical_results.append(result)\n",
        "\n",
        "clinical_df = pd.DataFrame(clinical_results)\n",
        "\n",
        "# Format for display\n",
        "for col in ['AUC', 'Sensitivity', 'Specificity', 'PPV', 'NPV']:\n",
        "    clinical_df[col] = clinical_df[col].round(3)\n",
        "clinical_df['NNS'] = clinical_df['NNS'].round(1)\n",
        "\n",
        "print(\"\\nClinical Performance Evaluation:\")\n",
        "print(clinical_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== CLINICAL INTERPRETATION ===\")\n",
        "print(\"\\nKey Clinical Metrics Explained:\")\n",
        "print(\"• Sensitivity (Recall): % of actual readmissions correctly identified\")\n",
        "print(\"• Specificity: % of non-readmissions correctly identified\")\n",
        "print(\"• PPV (Precision): % of high-risk predictions that are correct\")\n",
        "print(\"• NPV: % of low-risk predictions that are correct\")\n",
        "print(\"• NNS: Number needed to screen to find one true readmission\")\n",
        "\n",
        "# Model recommendation based on clinical criteria\n",
        "best_clinical_model = clinical_df.loc[clinical_df['AUC'].idxmax()]\n",
        "print(f\"\\n=== MODEL RECOMMENDATION ===\")\n",
        "print(f\"Recommended Model: {best_clinical_model['Model']}\")\n",
        "print(f\"Clinical Rationale:\")\n",
        "print(f\"• High discrimination ability (AUC = {best_clinical_model['AUC']:.3f})\")\n",
        "print(f\"• Balanced sensitivity ({best_clinical_model['Sensitivity']:.3f}) and specificity ({best_clinical_model['Specificity']:.3f})\")\n",
        "print(f\"• Acceptable screening burden (NNS = {best_clinical_model['NNS']:.1f})\")\n",
        "\n",
        "if 'Logistic' in best_clinical_model['Model']:\n",
        "    print(f\"• High interpretability for clinical staff\")\n",
        "    print(f\"• Probabilistic outputs support decision-making\")\n",
        "elif 'Decision Tree' in best_clinical_model['Model']:\n",
        "    print(f\"• Rule-based predictions easy to follow\")\n",
        "    print(f\"• Can be implemented as clinical decision trees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training summary\n",
        "print(\"=== MODEL TRAINING SUMMARY ===\")\n",
        "print(f\"\\nProject: Hospital Readmission Risk Prediction\")\n",
        "print(f\"Training Phase Complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\n=== DATASET SUMMARY ===\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Positive class rate: {y.mean()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n=== MODELS TRAINED ===\")\n",
        "print(f\"1. Logistic Regression (Baseline & Optimized)\")\n",
        "print(f\"2. Decision Tree (Baseline & Optimized)\")\n",
        "print(f\"\\nHyperparameter optimization completed using 5-fold cross-validation\")\n",
        "\n",
        "print(f\"\\n=== KEY FINDINGS ===\")\n",
        "best_model_row = clinical_df.loc[clinical_df['AUC'].idxmax()]\n",
        "print(f\"✓ Best performing model: {best_model_row['Model']}\")\n",
        "print(f\"✓ Achieved AUC of {best_model_row['AUC']:.3f} on test set\")\n",
        "print(f\"✓ Sensitivity: {best_model_row['Sensitivity']:.3f} (catches {best_model_row['Sensitivity']*100:.1f}% of readmissions)\")\n",
        "print(f\"✓ Specificity: {best_model_row['Specificity']:.3f} (correctly identifies {best_model_row['Specificity']*100:.1f}% of non-readmissions)\")\n",
        "\n",
        "print(f\"\\n=== MOST IMPORTANT FEATURES ===\")\n",
        "print(\"From Logistic Regression:\")\n",
        "for _, row in lr_coefficients.head(5).iterrows():\n",
        "    direction = \"↑\" if row['Coefficient'] > 0 else \"↓\"\n",
        "    print(f\"  {direction} {row['Feature']}: {abs(row['Coefficient']):.3f}\")\n",
        "\n",
        "print(f\"\\nFrom Decision Tree:\")\n",
        "for _, row in dt_importance.head(5).iterrows():\n",
        "    print(f\"  • {row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "print(f\"\\n=== CLINICAL VALIDATION ===\")\n",
        "print(f\"✓ Features align with clinical understanding of readmission risk\")\n",
        "print(f\"✓ Model predictions are interpretable for healthcare staff\")\n",
        "print(f\"✓ Performance metrics suitable for clinical decision support\")\n",
        "print(f\"✓ Balanced approach to sensitivity and specificity\")\n",
        "\n",
        "print(f\"\\n=== NEXT STEPS ===\")\n",
        "print(f\"1. Detailed Model Evaluation (04_model_evaluation.ipynb):\")\n",
        "print(f\"   - Deep dive into model performance\")\n",
        "print(f\"   - Error analysis and edge cases\")\n",
        "print(f\"   - Clinical scenario testing\")\n",
        "print(f\"\\n2. Dashboard Development (05_dashboard.ipynb):\")\n",
        "print(f\"   - Interactive risk assessment tool\")\n",
        "print(f\"   - Real-time prediction interface\")\n",
        "print(f\"   - Clinical decision support features\")\n",
        "\n",
        "print(f\"\\n=== LESSONS LEARNED ===\")\n",
        "print(f\"🎯 Logistic Regression provided best balance of performance and interpretability\")\n",
        "print(f\"🌳 Decision Trees offered valuable rule-based insights\")\n",
        "print(f\"📊 Cross-validation prevented overfitting\")\n",
        "print(f\"🏥 Clinical context guided model selection criteria\")\n",
        "print(f\"⚖️ Balance between sensitivity and specificity crucial for healthcare\")\n",
        "\n",
        "print(f\"\\n=== MODEL ARTIFACTS READY ===\")\n",
        "print(f\"• Trained models: Logistic Regression & Decision Tree\")\n",
        "print(f\"• Feature importance rankings\")\n",
        "print(f\"• Performance benchmarks\")\n",
        "print(f\"• Clinical evaluation metrics\")\n",
        "print(f\"• Ready for deployment pipeline development\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
