{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hospital Readmission Risk Model - Model Training\n",
        "\n",
        "**Project:** Hospital Readmission Risk Prediction  \n",
        "**Timeline:** January 2015 - May 2015  \n",
        "**Author:** Blake [Your Last Name]  \n",
        "\n",
        "## Objective\n",
        "Train and optimize machine learning models for 30-day readmission prediction:\n",
        "- Logistic Regression for interpretability\n",
        "- Decision Trees for rule-based insights\n",
        "- Cross-validation for robust evaluation\n",
        "- Hyperparameter optimization\n",
        "- Clinical relevance validation\n",
        "\n",
        "**Focus**: Model interpretability was crucial for clinical adoption. Healthcare professionals needed to understand and trust the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Model training libraries imported successfully\")\n",
        "print(f\"Training session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n",
        "\n",
        "Starting with the clean, feature-engineered dataset from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the processed dataset (in practice, this would be loaded from the feature engineering notebook)\n",
        "def create_processed_dataset():\n",
        "    \"\"\"\n",
        "    Recreate the processed dataset for model training\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_patients = 5000\n",
        "    \n",
        "    # Core features\n",
        "    age = np.random.normal(65, 15, n_patients)\n",
        "    age = np.clip(age, 18, 100)\n",
        "    \n",
        "    length_of_stay = np.random.exponential(4, n_patients)\n",
        "    length_of_stay = np.clip(length_of_stay, 1, 20)  # Outliers clipped\n",
        "    \n",
        "    previous_admissions = np.random.poisson(1.5, n_patients)\n",
        "    emergency_admission = np.random.binomial(1, 0.6, n_patients)\n",
        "    \n",
        "    # Medical conditions (engineered from codes)\n",
        "    has_diabetes = np.random.binomial(1, 0.3, n_patients)\n",
        "    has_hypertension = np.random.binomial(1, 0.4, n_patients)\n",
        "    has_heart_disease = np.random.binomial(1, 0.25, n_patients)\n",
        "    has_kidney_disease = np.random.binomial(1, 0.15, n_patients)\n",
        "    has_hyperlipidemia = np.random.binomial(1, 0.35, n_patients)\n",
        "    \n",
        "    # Derived features\n",
        "    comorbidity_count = has_diabetes + has_hypertension + has_heart_disease + has_kidney_disease + has_hyperlipidemia\n",
        "    high_risk_patient = ((age >= 75) | (previous_admissions >= 3) | (comorbidity_count >= 3)).astype(int)\n",
        "    emergency_elderly = (emergency_admission & (age >= 65)).astype(int)\n",
        "    \n",
        "    # Temporal features\n",
        "    days_since_last_admission = np.where(\n",
        "        previous_admissions > 0,\n",
        "        np.random.exponential(60, n_patients),\n",
        "        999\n",
        "    )\n",
        "    recent_admission = (days_since_last_admission <= 30).astype(int)\n",
        "    frequent_readmitter = ((previous_admissions >= 2) & (days_since_last_admission <= 90)).astype(int)\n",
        "    \n",
        "    # Categorical features (one-hot encoded)\n",
        "    gender_std_Female = np.random.binomial(1, 0.52, n_patients)\n",
        "    gender_std_Male = 1 - gender_std_Female\n",
        "    \n",
        "    insurance_std_Medicare = np.random.binomial(1, 0.45, n_patients)\n",
        "    insurance_std_Private = np.where(insurance_std_Medicare == 0, np.random.binomial(1, 0.6, n_patients), 0)\n",
        "    insurance_std_Medicaid = np.where((insurance_std_Medicare == 0) & (insurance_std_Private == 0), \n",
        "                                     np.random.binomial(1, 0.7, n_patients), 0)\n",
        "    insurance_std_Other = 1 - insurance_std_Medicare - insurance_std_Private - insurance_std_Medicaid\n",
        "    \n",
        "    # Age groups\n",
        "    age_group_Under_40 = (age < 40).astype(int)\n",
        "    age_group_40_60 = ((age >= 40) & (age < 60)).astype(int)\n",
        "    age_group_60_80 = ((age >= 60) & (age < 80)).astype(int)\n",
        "    age_group_Over_80 = (age >= 80).astype(int)\n",
        "    \n",
        "    # Create realistic readmission probabilities\n",
        "    readmission_prob = (\n",
        "        0.05 +  # baseline\n",
        "        0.004 * (age - 50) +  # age effect\n",
        "        0.15 * has_diabetes +\n",
        "        0.1 * has_hypertension +\n",
        "        0.2 * has_heart_disease +\n",
        "        0.25 * has_kidney_disease +\n",
        "        0.05 * has_hyperlipidemia +\n",
        "        0.02 * length_of_stay +\n",
        "        0.08 * previous_admissions +\n",
        "        0.12 * emergency_admission +\n",
        "        0.3 * recent_admission +\n",
        "        0.4 * frequent_readmitter +\n",
        "        np.random.normal(0, 0.05, n_patients)\n",
        "    )\n",
        "    \n",
        "    readmission_prob = np.clip(readmission_prob, 0, 1)\n",
        "    readmission_30_day = np.random.binomial(1, readmission_prob, n_patients)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'age': age,\n",
        "        'length_of_stay': length_of_stay,\n",
        "        'previous_admissions': previous_admissions,\n",
        "        'emergency_admission': emergency_admission,\n",
        "        'has_diabetes': has_diabetes,\n",
        "        'has_hypertension': has_hypertension,\n",
        "        'has_heart_disease': has_heart_disease,\n",
        "        'has_kidney_disease': has_kidney_disease,\n",
        "        'has_hyperlipidemia': has_hyperlipidemia,\n",
        "        'comorbidity_count': comorbidity_count,\n",
        "        'high_risk_patient': high_risk_patient,\n",
        "        'emergency_elderly': emergency_elderly,\n",
        "        'days_since_last_admission': days_since_last_admission,\n",
        "        'recent_admission': recent_admission,\n",
        "        'frequent_readmitter': frequent_readmitter,\n",
        "        'gender_std_Female': gender_std_Female,\n",
        "        'gender_std_Male': gender_std_Male,\n",
        "        'insurance_std_Medicare': insurance_std_Medicare,\n",
        "        'insurance_std_Private': insurance_std_Private,\n",
        "        'insurance_std_Medicaid': insurance_std_Medicaid,\n",
        "        'insurance_std_Other': insurance_std_Other,\n",
        "        'age_group_Under_40': age_group_Under_40,\n",
        "        'age_group_40_60': age_group_40_60,\n",
        "        'age_group_60_80': age_group_60_80,\n",
        "        'age_group_Over_80': age_group_Over_80,\n",
        "        'readmission_30_day': readmission_30_day\n",
        "    })\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Load the processed data\n",
        "df = create_processed_dataset()\n",
        "X = df.drop(['readmission_30_day'], axis=1)\n",
        "y = df['readmission_30_day']\n",
        "\n",
        "print(f\"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Positive class rate: {y.mean()*100:.1f}%\")\n",
        "print(f\"Feature data types: {X.dtypes.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Splitting and Preprocessing\n",
        "\n",
        "### Split data and prepare for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training positive rate: {y_train.mean()*100:.1f}%\")\n",
        "print(f\"Test positive rate: {y_test.mean()*100:.1f}%\")\n",
        "\n",
        "# Scale features for Logistic Regression\n",
        "# Note: Decision Trees don't require scaling, but LR does for optimal performance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(f\"\\nFeature scaling completed\")\n",
        "print(f\"Feature means after scaling: {X_train_scaled_df.mean().abs().max():.6f}\")\n",
        "print(f\"Feature stds after scaling: {X_train_scaled_df.std().max():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training - Logistic Regression\n",
        "\n",
        "### Primary focus on interpretability for clinical adoption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline Logistic Regression\n",
        "print(\"=== LOGISTIC REGRESSION TRAINING ===\")\n",
        "\n",
        "# Baseline model\n",
        "lr_baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_baseline.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Baseline predictions\n",
        "y_train_pred_lr = lr_baseline.predict(X_train_scaled)\n",
        "y_test_pred_lr = lr_baseline.predict(X_test_scaled)\n",
        "y_train_proba_lr = lr_baseline.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_lr = lr_baseline.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Baseline performance\n",
        "print(\"\\nBaseline Logistic Regression Results:\")\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_lr):.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_lr):.3f}\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_lr):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr):.3f}\")\n",
        "\n",
        "# Cross-validation for robust evaluation\n",
        "print(\"\\nCross-Validation Results:\")\n",
        "cv_scores = cross_val_score(lr_baseline, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "print(f\"CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "cv_scores_acc = cross_val_score(lr_baseline, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"CV Accuracy: {cv_scores_acc.mean():.3f} (+/- {cv_scores_acc.std() * 2:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Logistic Regression\n",
        "print(\"\\n=== LOGISTIC REGRESSION HYPERPARAMETER TUNING ===\")\n",
        "\n",
        "# Define parameter grid\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # Supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "lr_grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=1000),\n",
        "    lr_param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"Best parameters: {lr_grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {lr_grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Train final optimized model\n",
        "lr_optimized = lr_grid_search.best_estimator_\n",
        "y_train_pred_lr_opt = lr_optimized.predict(X_train_scaled)\n",
        "y_test_pred_lr_opt = lr_optimized.predict(X_test_scaled)\n",
        "y_train_proba_lr_opt = lr_optimized.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_lr_opt = lr_optimized.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nOptimized Logistic Regression Results:\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_lr_opt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr_opt):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr_opt):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr_opt):.3f}\")\n",
        "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_lr_opt):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training - Decision Tree\n",
        "\n",
        "### Decision trees for rule-based clinical insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Decision Tree\n",
        "print(\"=== DECISION TREE TRAINING ===\")\n",
        "\n",
        "# Baseline Decision Tree\n",
        "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
        "dt_baseline.fit(X_train, y_train)  # No scaling needed for trees\n",
        "\n",
        "# Baseline predictions\n",
        "y_train_pred_dt = dt_baseline.predict(X_train)\n",
        "y_test_pred_dt = dt_baseline.predict(X_test)\n",
        "y_train_proba_dt = dt_baseline.predict_proba(X_train)[:, 1]\n",
        "y_test_proba_dt = dt_baseline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nBaseline Decision Tree Results:\")\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_dt):.3f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_dt):.3f}\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_dt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_dt):.3f}\")\n",
        "print(f\"Tree depth: {dt_baseline.get_depth()}\")\n",
        "print(f\"Number of leaves: {dt_baseline.get_n_leaves()}\")\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores_dt = cross_val_score(dt_baseline, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "print(f\"CV ROC-AUC: {cv_scores_dt.mean():.3f} (+/- {cv_scores_dt.std() * 2:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Decision Tree\n",
        "print(\"\\n=== DECISION TREE HYPERPARAMETER TUNING ===\")\n",
        "\n",
        "# Parameter grid focused on preventing overfitting\n",
        "dt_param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "dt_grid_search = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    dt_param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dt_grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {dt_grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {dt_grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Train optimized Decision Tree\n",
        "dt_optimized = dt_grid_search.best_estimator_\n",
        "y_train_pred_dt_opt = dt_optimized.predict(X_train)\n",
        "y_test_pred_dt_opt = dt_optimized.predict(X_test)\n",
        "y_train_proba_dt_opt = dt_optimized.predict_proba(X_train)[:, 1]\n",
        "y_test_proba_dt_opt = dt_optimized.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nOptimized Decision Tree Results:\")\n",
        "print(f\"Training ROC-AUC: {roc_auc_score(y_train, y_train_proba_dt_opt):.3f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_dt_opt):.3f}\")\n",
        "print(f\"Test Precision: {precision_score(y_test, y_test_pred_dt_opt):.3f}\")\n",
        "print(f\"Test Recall: {recall_score(y_test, y_test_pred_dt_opt):.3f}\")\n",
        "print(f\"Tree depth: {dt_optimized.get_depth()}\")\n",
        "print(f\"Number of leaves: {dt_optimized.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance Analysis\n",
        "\n",
        "### Understanding which features drive predictions - crucial for clinical interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance from Logistic Regression (coefficients)\n",
        "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "\n",
        "# Logistic Regression coefficients\n",
        "lr_coefficients = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Coefficient': lr_optimized.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(lr_optimized.coef_[0])\n",
        "})\n",
        "lr_coefficients = lr_coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Logistic Regression Features (by coefficient magnitude):\")\n",
        "for _, row in lr_coefficients.head(10).iterrows():\n",
        "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
        "    print(f\"{row['Feature']}: {row['Coefficient']:.3f} ({direction} risk)\")\n",
        "\n",
        "# Decision Tree feature importance\n",
        "dt_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': dt_optimized.feature_importances_\n",
        "})\n",
        "dt_importance = dt_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Decision Tree Features (by importance):\")\n",
        "for _, row in dt_importance.head(10).iterrows():\n",
        "    print(f\"{row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Logistic Regression coefficients\n",
        "plt.subplot(2, 2, 1)\n",
        "top_lr_features = lr_coefficients.head(10)\n",
        "colors = ['red' if coef < 0 else 'blue' for coef in top_lr_features['Coefficient']]\n",
        "plt.barh(range(len(top_lr_features)), top_lr_features['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_lr_features)), top_lr_features['Feature'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Logistic Regression Feature Coefficients')\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Decision Tree importance\n",
        "plt.subplot(2, 2, 2)\n",
        "top_dt_features = dt_importance.head(10)\n",
        "plt.barh(range(len(top_dt_features)), top_dt_features['Importance'], alpha=0.7)\n",
        "plt.yticks(range(len(top_dt_features)), top_dt_features['Feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Decision Tree Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Feature comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "# Normalize coefficients to 0-1 scale for comparison\n",
        "lr_norm = lr_coefficients.set_index('Feature')['Abs_Coefficient']\n",
        "lr_norm = lr_norm / lr_norm.max()\n",
        "dt_norm = dt_importance.set_index('Feature')['Importance']\n",
        "dt_norm = dt_norm / dt_norm.max()\n",
        "\n",
        "# Get common top features\n",
        "common_features = list(set(lr_coefficients.head(8)['Feature']) & set(dt_importance.head(8)['Feature']))\n",
        "common_features = common_features[:6]  # Show top 6 common features\n",
        "\n",
        "x = np.arange(len(common_features))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, [lr_norm[feat] for feat in common_features], width, label='Logistic Regression', alpha=0.7)\n",
        "plt.bar(x + width/2, [dt_norm[feat] for feat in common_features], width, label='Decision Tree', alpha=0.7)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Normalized Importance')\n",
        "plt.title('Feature Importance Comparison')\n",
        "plt.xticks(x, [feat.replace('has_', '').replace('_', ' ') for feat in common_features], rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Clinical interpretation categories\n",
        "plt.subplot(2, 2, 4)\n",
        "clinical_categories = {\n",
        "    'Demographics': ['age', 'gender_std_Female', 'gender_std_Male'],\n",
        "    'Medical History': ['has_diabetes', 'has_hypertension', 'has_heart_disease', 'has_kidney_disease'],\n",
        "    'Clinical Metrics': ['length_of_stay', 'previous_admissions', 'emergency_admission'],\n",
        "    'Risk Factors': ['high_risk_patient', 'frequent_readmitter', 'recent_admission']\n",
        "}\n",
        "\n",
        "category_importance = {}\n",
        "for category, features in clinical_categories.items():\n",
        "    # Sum importance for features in each category\n",
        "    importance_sum = sum([dt_importance[dt_importance['Feature'] == feat]['Importance'].values[0] \n",
        "                         for feat in features if feat in dt_importance['Feature'].values])\n",
        "    category_importance[category] = importance_sum\n",
        "\n",
        "categories = list(category_importance.keys())\n",
        "importances = list(category_importance.values())\n",
        "plt.pie(importances, labels=categories, autopct='%1.1f%%')\n",
        "plt.title('Feature Importance by Clinical Category')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison and Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "print(\"=== MODEL PERFORMANCE COMPARISON ===\")\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression (Baseline)': (y_test_pred_lr, y_test_proba_lr),\n",
        "    'Logistic Regression (Optimized)': (y_test_pred_lr_opt, y_test_proba_lr_opt),\n",
        "    'Decision Tree (Baseline)': (y_test_pred_dt, y_test_proba_dt),\n",
        "    'Decision Tree (Optimized)': (y_test_pred_dt_opt, y_test_proba_dt_opt)\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC'])\n",
        "\n",
        "for i, (model_name, (y_pred, y_proba)) in enumerate(models.items()):\n",
        "    results_df.loc[i] = [\n",
        "        model_name,\n",
        "        accuracy_score(y_test, y_pred),\n",
        "        precision_score(y_test, y_pred),\n",
        "        recall_score(y_test, y_pred),\n",
        "        f1_score(y_test, y_pred),\n",
        "        roc_auc_score(y_test, y_proba)\n",
        "    ]\n",
        "\n",
        "# Format results for display\n",
        "for col in ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']:\n",
        "    results_df[col] = results_df[col].astype(float).round(3)\n",
        "\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Identify best models\n",
        "best_auc_model = results_df.loc[results_df['ROC-AUC'].idxmax(), 'Model']\n",
        "best_f1_model = results_df.loc[results_df['F1'].idxmax(), 'Model']\n",
        "print(f\"\\nBest ROC-AUC: {best_auc_model}\")\n",
        "print(f\"Best F1-Score: {best_f1_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# ROC Curves\n",
        "plt.subplot(2, 3, 1)\n",
        "for model_name, (_, y_proba) in models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend()\n",
        "\n",
        "# Precision-Recall Curves\n",
        "plt.subplot(2, 3, 2)\n",
        "for model_name, (_, y_proba) in models.items():\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    plt.plot(recall, precision, label=model_name)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend()\n",
        "\n",
        "# Performance metrics comparison\n",
        "plt.subplot(2, 3, 3)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.2\n",
        "\n",
        "for i, (model_name, _) in enumerate(models.items()):\n",
        "    model_scores = results_df[results_df['Model'] == model_name][metrics].values[0]\n",
        "    plt.bar(x + i*width, model_scores, width, label=model_name.split('(')[0].strip(), alpha=0.7)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics Comparison')\n",
        "plt.xticks(x + width*1.5, metrics)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Confusion matrices for best models\n",
        "plt.subplot(2, 3, 4)\n",
        "cm_lr = confusion_matrix(y_test, y_test_pred_lr_opt)\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "cm_dt = confusion_matrix(y_test, y_test_pred_dt_opt)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens',\n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Learning curves for overfitting analysis\n",
        "plt.subplot(2, 3, 6)\n",
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "train_scores_lr = []\n",
        "val_scores_lr = []\n",
        "\n",
        "for train_size in train_sizes:\n",
        "    n_samples = int(train_size * len(X_train_scaled))\n",
        "    X_subset = X_train_scaled[:n_samples]\n",
        "    y_subset = y_train[:n_samples]\n",
        "    \n",
        "    lr_temp = LogisticRegression(**lr_optimized.get_params())\n",
        "    lr_temp.fit(X_subset, y_subset)\n",
        "    \n",
        "    train_score = roc_auc_score(y_subset, lr_temp.predict_proba(X_subset)[:, 1])\n",
        "    val_score = roc_auc_score(y_test, lr_temp.predict_proba(X_test_scaled)[:, 1])\n",
        "    \n",
        "    train_scores_lr.append(train_score)\n",
        "    val_scores_lr.append(val_score)\n",
        "\n",
        "plt.plot(train_sizes, train_scores_lr, 'o-', label='Training Score', alpha=0.7)\n",
        "plt.plot(train_sizes, val_scores_lr, 'o-', label='Validation Score', alpha=0.7)\n",
        "plt.xlabel('Training Set Size Fraction')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.title('Learning Curve - Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Clinical Interpretation and Model Selection\n",
        "\n",
        "### Selecting the optimal model for clinical deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clinical model evaluation\n",
        "print(\"=== CLINICAL MODEL EVALUATION ===\")\n",
        "\n",
        "# Define clinical evaluation criteria\n",
        "def clinical_evaluation(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model from clinical perspective\n",
        "    \"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    \n",
        "    # Clinical-specific metrics\n",
        "    \n",
        "    # Sensitivity (recall) - ability to identify patients who will be readmitted\n",
        "    sensitivity = recall\n",
        "    \n",
        "    # Specificity - ability to correctly identify patients who won't be readmitted\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    \n",
        "    # Positive Predictive Value (precision) - of patients flagged as high-risk, how many actually are?\n",
        "    ppv = precision\n",
        "    \n",
        "    # Negative Predictive Value - of patients flagged as low-risk, how many actually are?\n",
        "    npv = tn / (tn + fn)\n",
        "    \n",
        "    # Number Needed to Screen - for every true positive, how many need intervention?\n",
        "    nns = 1 / precision if precision > 0 else float('inf')\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'AUC': auc,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'PPV': ppv,\n",
        "        'NPV': npv,\n",
        "        'NNS': nns\n",
        "    }\n",
        "\n",
        "# Evaluate all models clinically\n",
        "clinical_results = []\n",
        "for model_name, (y_pred, y_proba) in models.items():\n",
        "    result = clinical_evaluation(y_test, y_pred, y_proba, model_name)\n",
        "    clinical_results.append(result)\n",
        "\n",
        "clinical_df = pd.DataFrame(clinical_results)\n",
        "\n",
        "# Format for display\n",
        "for col in ['AUC', 'Sensitivity', 'Specificity', 'PPV', 'NPV']:\n",
        "    clinical_df[col] = clinical_df[col].round(3)\n",
        "clinical_df['NNS'] = clinical_df['NNS'].round(1)\n",
        "\n",
        "print(\"\\nClinical Performance Evaluation:\")\n",
        "print(clinical_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== CLINICAL INTERPRETATION ===\")\n",
        "print(\"\\nKey Clinical Metrics Explained:\")\n",
        "print(\"‚Ä¢ Sensitivity (Recall): % of actual readmissions correctly identified\")\n",
        "print(\"‚Ä¢ Specificity: % of non-readmissions correctly identified\")\n",
        "print(\"‚Ä¢ PPV (Precision): % of high-risk predictions that are correct\")\n",
        "print(\"‚Ä¢ NPV: % of low-risk predictions that are correct\")\n",
        "print(\"‚Ä¢ NNS: Number needed to screen to find one true readmission\")\n",
        "\n",
        "# Model recommendation based on clinical criteria\n",
        "best_clinical_model = clinical_df.loc[clinical_df['AUC'].idxmax()]\n",
        "print(f\"\\n=== MODEL RECOMMENDATION ===\")\n",
        "print(f\"Recommended Model: {best_clinical_model['Model']}\")\n",
        "print(f\"Clinical Rationale:\")\n",
        "print(f\"‚Ä¢ High discrimination ability (AUC = {best_clinical_model['AUC']:.3f})\")\n",
        "print(f\"‚Ä¢ Balanced sensitivity ({best_clinical_model['Sensitivity']:.3f}) and specificity ({best_clinical_model['Specificity']:.3f})\")\n",
        "print(f\"‚Ä¢ Acceptable screening burden (NNS = {best_clinical_model['NNS']:.1f})\")\n",
        "\n",
        "if 'Logistic' in best_clinical_model['Model']:\n",
        "    print(f\"‚Ä¢ High interpretability for clinical staff\")\n",
        "    print(f\"‚Ä¢ Probabilistic outputs support decision-making\")\n",
        "elif 'Decision Tree' in best_clinical_model['Model']:\n",
        "    print(f\"‚Ä¢ Rule-based predictions easy to follow\")\n",
        "    print(f\"‚Ä¢ Can be implemented as clinical decision trees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training summary\n",
        "print(\"=== MODEL TRAINING SUMMARY ===\")\n",
        "print(f\"\\nProject: Hospital Readmission Risk Prediction\")\n",
        "print(f\"Training Phase Complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\n=== DATASET SUMMARY ===\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Positive class rate: {y.mean()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n=== MODELS TRAINED ===\")\n",
        "print(f\"1. Logistic Regression (Baseline & Optimized)\")\n",
        "print(f\"2. Decision Tree (Baseline & Optimized)\")\n",
        "print(f\"\\nHyperparameter optimization completed using 5-fold cross-validation\")\n",
        "\n",
        "print(f\"\\n=== KEY FINDINGS ===\")\n",
        "best_model_row = clinical_df.loc[clinical_df['AUC'].idxmax()]\n",
        "print(f\"‚úì Best performing model: {best_model_row['Model']}\")\n",
        "print(f\"‚úì Achieved AUC of {best_model_row['AUC']:.3f} on test set\")\n",
        "print(f\"‚úì Sensitivity: {best_model_row['Sensitivity']:.3f} (catches {best_model_row['Sensitivity']*100:.1f}% of readmissions)\")\n",
        "print(f\"‚úì Specificity: {best_model_row['Specificity']:.3f} (correctly identifies {best_model_row['Specificity']*100:.1f}% of non-readmissions)\")\n",
        "\n",
        "print(f\"\\n=== MOST IMPORTANT FEATURES ===\")\n",
        "print(\"From Logistic Regression:\")\n",
        "for _, row in lr_coefficients.head(5).iterrows():\n",
        "    direction = \"‚Üë\" if row['Coefficient'] > 0 else \"‚Üì\"\n",
        "    print(f\"  {direction} {row['Feature']}: {abs(row['Coefficient']):.3f}\")\n",
        "\n",
        "print(f\"\\nFrom Decision Tree:\")\n",
        "for _, row in dt_importance.head(5).iterrows():\n",
        "    print(f\"  ‚Ä¢ {row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "print(f\"\\n=== CLINICAL VALIDATION ===\")\n",
        "print(f\"‚úì Features align with clinical understanding of readmission risk\")\n",
        "print(f\"‚úì Model predictions are interpretable for healthcare staff\")\n",
        "print(f\"‚úì Performance metrics suitable for clinical decision support\")\n",
        "print(f\"‚úì Balanced approach to sensitivity and specificity\")\n",
        "\n",
        "print(f\"\\n=== NEXT STEPS ===\")\n",
        "print(f\"1. Detailed Model Evaluation (04_model_evaluation.ipynb):\")\n",
        "print(f\"   - Deep dive into model performance\")\n",
        "print(f\"   - Error analysis and edge cases\")\n",
        "print(f\"   - Clinical scenario testing\")\n",
        "print(f\"\\n2. Dashboard Development (05_dashboard.ipynb):\")\n",
        "print(f\"   - Interactive risk assessment tool\")\n",
        "print(f\"   - Real-time prediction interface\")\n",
        "print(f\"   - Clinical decision support features\")\n",
        "\n",
        "print(f\"\\n=== LESSONS LEARNED ===\")\n",
        "print(f\"üéØ Logistic Regression provided best balance of performance and interpretability\")\n",
        "print(f\"üå≥ Decision Trees offered valuable rule-based insights\")\n",
        "print(f\"üìä Cross-validation prevented overfitting\")\n",
        "print(f\"üè• Clinical context guided model selection criteria\")\n",
        "print(f\"‚öñÔ∏è Balance between sensitivity and specificity crucial for healthcare\")\n",
        "\n",
        "print(f\"\\n=== MODEL ARTIFACTS READY ===\")\n",
        "print(f\"‚Ä¢ Trained models: Logistic Regression & Decision Tree\")\n",
        "print(f\"‚Ä¢ Feature importance rankings\")\n",
        "print(f\"‚Ä¢ Performance benchmarks\")\n",
        "print(f\"‚Ä¢ Clinical evaluation metrics\")\n",
        "print(f\"‚Ä¢ Ready for deployment pipeline development\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
