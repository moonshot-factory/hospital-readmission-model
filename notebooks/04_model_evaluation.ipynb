{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hospital Readmission Risk Model - Model Evaluation\n",
        "\n",
        "**Project:** Hospital Readmission Risk Prediction  \n",
        "**Timeline:** January 2015 - May 2015  \n",
        "**Author:** Blake Sonnier  \n",
        "\n",
        "## Objective\n",
        "Comprehensive evaluation of trained models with focus on clinical applicability:\n",
        "- Detailed performance analysis using ROC-AUC and Precision-Recall curves\n",
        "- Error analysis and edge case identification\n",
        "- Clinical scenario testing and validation\n",
        "- Model interpretability for healthcare professionals\n",
        "- Bias detection and fairness assessment\n",
        "\n",
        "**Clinical Focus**: Ensuring the model performs reliably across different patient populations and clinical scenarios encountered in Southeast Texas hospitals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/h/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve,\n",
        "    confusion_matrix, classification_report,\n",
        "    average_precision_score, brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "\n",
        "print(\"Model evaluation libraries imported successfully\")\n",
        "print(f\"Evaluation session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Trained Models and Test Data\n",
        "\n",
        "Recreating the best-performing models from the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the processed dataset and trained models\n",
        "def setup_evaluation_environment():\n",
        "    \"\"\"\n",
        "    Recreate the dataset and trained models for evaluation\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_patients = 5000\n",
        "    \n",
        "    # Recreate the feature set\n",
        "    age = np.random.normal(65, 15, n_patients)\n",
        "    age = np.clip(age, 18, 100)\n",
        "    \n",
        "    length_of_stay = np.random.exponential(4, n_patients)\n",
        "    length_of_stay = np.clip(length_of_stay, 1, 20)\n",
        "    \n",
        "    previous_admissions = np.random.poisson(1.5, n_patients)\n",
        "    emergency_admission = np.random.binomial(1, 0.6, n_patients)\n",
        "    \n",
        "    # Medical conditions\n",
        "    has_diabetes = np.random.binomial(1, 0.3, n_patients)\n",
        "    has_hypertension = np.random.binomial(1, 0.4, n_patients)\n",
        "    has_heart_disease = np.random.binomial(1, 0.25, n_patients)\n",
        "    has_kidney_disease = np.random.binomial(1, 0.15, n_patients)\n",
        "    has_hyperlipidemia = np.random.binomial(1, 0.35, n_patients)\n",
        "    \n",
        "    # Derived features\n",
        "    comorbidity_count = has_diabetes + has_hypertension + has_heart_disease + has_kidney_disease + has_hyperlipidemia\n",
        "    high_risk_patient = ((age >= 75) | (previous_admissions >= 3) | (comorbidity_count >= 3)).astype(int)\n",
        "    emergency_elderly = (emergency_admission & (age >= 65)).astype(int)\n",
        "    \n",
        "    # Temporal features\n",
        "    days_since_last_admission = np.where(\n",
        "        previous_admissions > 0,\n",
        "        np.random.exponential(60, n_patients),\n",
        "        999\n",
        "    )\n",
        "    recent_admission = (days_since_last_admission <= 30).astype(int)\n",
        "    frequent_readmitter = ((previous_admissions >= 2) & (days_since_last_admission <= 90)).astype(int)\n",
        "    \n",
        "    # Categorical features\n",
        "    gender_std_Female = np.random.binomial(1, 0.52, n_patients)\n",
        "    gender_std_Male = 1 - gender_std_Female\n",
        "    \n",
        "    insurance_std_Medicare = np.random.binomial(1, 0.45, n_patients)\n",
        "    insurance_std_Private = np.where(insurance_std_Medicare == 0, np.random.binomial(1, 0.6, n_patients), 0)\n",
        "    insurance_std_Medicaid = np.where((insurance_std_Medicare == 0) & (insurance_std_Private == 0), \n",
        "                                     np.random.binomial(1, 0.7, n_patients), 0)\n",
        "    insurance_std_Other = 1 - insurance_std_Medicare - insurance_std_Private - insurance_std_Medicaid\n",
        "    \n",
        "    # Age groups\n",
        "    age_group_Under_40 = (age < 40).astype(int)\n",
        "    age_group_40_60 = ((age >= 40) & (age < 60)).astype(int)\n",
        "    age_group_60_80 = ((age >= 60) & (age < 80)).astype(int)\n",
        "    age_group_Over_80 = (age >= 80).astype(int)\n",
        "    \n",
        "    # Create realistic readmission probabilities\n",
        "    readmission_prob = (\n",
        "        0.05 +  # baseline\n",
        "        0.004 * (age - 50) +\n",
        "        0.15 * has_diabetes +\n",
        "        0.1 * has_hypertension +\n",
        "        0.2 * has_heart_disease +\n",
        "        0.25 * has_kidney_disease +\n",
        "        0.05 * has_hyperlipidemia +\n",
        "        0.02 * length_of_stay +\n",
        "        0.08 * previous_admissions +\n",
        "        0.12 * emergency_admission +\n",
        "        0.3 * recent_admission +\n",
        "        0.4 * frequent_readmitter +\n",
        "        np.random.normal(0, 0.05, n_patients)\n",
        "    )\n",
        "    \n",
        "    readmission_prob = np.clip(readmission_prob, 0, 1)\n",
        "    readmission_30_day = np.random.binomial(1, readmission_prob, n_patients)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'age': age,\n",
        "        'length_of_stay': length_of_stay,\n",
        "        'previous_admissions': previous_admissions,\n",
        "        'emergency_admission': emergency_admission,\n",
        "        'has_diabetes': has_diabetes,\n",
        "        'has_hypertension': has_hypertension,\n",
        "        'has_heart_disease': has_heart_disease,\n",
        "        'has_kidney_disease': has_kidney_disease,\n",
        "        'has_hyperlipidemia': has_hyperlipidemia,\n",
        "        'comorbidity_count': comorbidity_count,\n",
        "        'high_risk_patient': high_risk_patient,\n",
        "        'emergency_elderly': emergency_elderly,\n",
        "        'days_since_last_admission': days_since_last_admission,\n",
        "        'recent_admission': recent_admission,\n",
        "        'frequent_readmitter': frequent_readmitter,\n",
        "        'gender_std_Female': gender_std_Female,\n",
        "        'gender_std_Male': gender_std_Male,\n",
        "        'insurance_std_Medicare': insurance_std_Medicare,\n",
        "        'insurance_std_Private': insurance_std_Private,\n",
        "        'insurance_std_Medicaid': insurance_std_Medicaid,\n",
        "        'insurance_std_Other': insurance_std_Other,\n",
        "        'age_group_Under_40': age_group_Under_40,\n",
        "        'age_group_40_60': age_group_40_60,\n",
        "        'age_group_60_80': age_group_60_80,\n",
        "        'age_group_Over_80': age_group_Over_80,\n",
        "        'readmission_30_day': readmission_30_day\n",
        "    })\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Setup evaluation environment\n",
        "df = setup_evaluation_environment()\n",
        "X = df.drop(['readmission_30_day'], axis=1)\n",
        "y = df['readmission_30_day']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train optimized models (recreating best parameters from training phase)\n",
        "lr_model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "dt_model = DecisionTreeClassifier(max_depth=7, min_samples_split=10, min_samples_leaf=5, \n",
        "                                 criterion='gini', random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Models trained successfully\")\n",
        "print(f\"Dataset: {X.shape[0]} patients, {X.shape[1]} features\")\n",
        "print(f\"Test set: {X_test.shape[0]} patients\")\n",
        "print(f\"Readmission rate: {y.mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Comprehensive Performance Analysis\n",
        "\n",
        "### ROC-AUC and Precision-Recall Curves with Clinical Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for both models\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "y_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"=== COMPREHENSIVE PERFORMANCE ANALYSIS ===\")\n",
        "\n",
        "# Calculate detailed metrics\n",
        "def calculate_detailed_metrics(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive performance metrics\n",
        "    \"\"\"\n",
        "    # Basic classification metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    \n",
        "    # ROC and PR metrics\n",
        "    roc_auc = roc_auc_score(y_true, y_proba)\n",
        "    pr_auc = average_precision_score(y_true, y_proba)\n",
        "    \n",
        "    # Confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    \n",
        "    # Clinical metrics\n",
        "    sensitivity = tp / (tp + fn)  # Same as recall\n",
        "    specificity = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Same as precision\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    \n",
        "    # Clinical utility metrics\n",
        "    nns = 1 / precision if precision > 0 else float('inf')  # Number needed to screen\n",
        "    likelihood_ratio_pos = sensitivity / (1 - specificity) if specificity < 1 else float('inf')\n",
        "    likelihood_ratio_neg = (1 - sensitivity) / specificity if specificity > 0 else float('inf')\n",
        "    \n",
        "    # Calibration metric\n",
        "    brier_score = brier_score_loss(y_true, y_proba)\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1': f1,\n",
        "        'ROC_AUC': roc_auc,\n",
        "        'PR_AUC': pr_auc,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'PPV': ppv,\n",
        "        'NPV': npv,\n",
        "        'NNS': nns,\n",
        "        'LR_pos': likelihood_ratio_pos,\n",
        "        'LR_neg': likelihood_ratio_neg,\n",
        "        'Brier_Score': brier_score,\n",
        "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn\n",
        "    }\n",
        "\n",
        "# Calculate metrics for both models\n",
        "lr_metrics = calculate_detailed_metrics(y_test, y_pred_lr, y_proba_lr, 'Logistic Regression')\n",
        "dt_metrics = calculate_detailed_metrics(y_test, y_pred_dt, y_proba_dt, 'Decision Tree')\n",
        "\n",
        "# Create comparison DataFrame\n",
        "metrics_df = pd.DataFrame([lr_metrics, dt_metrics])\n",
        "\n",
        "# Format for display\n",
        "display_cols = ['Model', 'ROC_AUC', 'PR_AUC', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1', 'NNS']\n",
        "display_df = metrics_df[display_cols].copy()\n",
        "for col in display_cols[1:-1]:  # Skip Model and NNS\n",
        "    display_df[col] = display_df[col].round(3)\n",
        "display_df['NNS'] = display_df['NNS'].round(1)\n",
        "\n",
        "print(\"\\nDetailed Performance Comparison:\")\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Clinical interpretation\n",
        "print(f\"\\n=== CLINICAL INTERPRETATION ===\")\n",
        "for _, row in metrics_df.iterrows():\n",
        "    print(f\"\\n{row['Model']}:\")\n",
        "    print(f\"  • Catches {row['Sensitivity']*100:.1f}% of actual readmissions (Sensitivity)\")\n",
        "    print(f\"  • Correctly identifies {row['Specificity']*100:.1f}% of non-readmissions (Specificity)\")\n",
        "    print(f\"  • {row['PPV']*100:.1f}% of high-risk predictions are correct (PPV)\")\n",
        "    print(f\"  • Need to screen {row['NNS']:.1f} patients to find 1 true readmission (NNS)\")\n",
        "    print(f\"  • Model calibration: Brier Score = {row['Brier_Score']:.3f} (lower is better)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization of model performance\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# ROC Curves\n",
        "plt.subplot(3, 3, 1)\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)\n",
        "\n",
        "plt.plot(fpr_lr, tpr_lr, 'b-', label=f'Logistic Regression (AUC = {lr_metrics[\"ROC_AUC\"]:.3f})', linewidth=2)\n",
        "plt.plot(fpr_dt, tpr_dt, 'r-', label=f'Decision Tree (AUC = {dt_metrics[\"ROC_AUC\"]:.3f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curves\n",
        "plt.subplot(3, 3, 2)\n",
        "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_proba_lr)\n",
        "precision_dt, recall_dt, _ = precision_recall_curve(y_test, y_proba_dt)\n",
        "\n",
        "plt.plot(recall_lr, precision_lr, 'b-', label=f'Logistic Regression (AP = {lr_metrics[\"PR_AUC\"]:.3f})', linewidth=2)\n",
        "plt.plot(recall_dt, precision_dt, 'r-', label=f'Decision Tree (AP = {dt_metrics[\"PR_AUC\"]:.3f})', linewidth=2)\n",
        "plt.axhline(y=y_test.mean(), color='k', linestyle='--', alpha=0.5, label=f'Baseline ({y_test.mean():.3f})')\n",
        "plt.xlabel('Recall (Sensitivity)')\n",
        "plt.ylabel('Precision (PPV)')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrices\n",
        "plt.subplot(3, 3, 3)\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Logistic Regression\\nConfusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.subplot(3, 3, 4)\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Reds',\n",
        "           xticklabels=['No Readmission', 'Readmission'],\n",
        "           yticklabels=['No Readmission', 'Readmission'])\n",
        "plt.title('Decision Tree\\nConfusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Prediction probability distributions\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.hist(y_proba_lr[y_test == 0], bins=30, alpha=0.7, label='No Readmission', density=True)\n",
        "plt.hist(y_proba_lr[y_test == 1], bins=30, alpha=0.7, label='Readmission', density=True)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('LR: Probability Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.hist(y_proba_dt[y_test == 0], bins=30, alpha=0.7, label='No Readmission', density=True)\n",
        "plt.hist(y_proba_dt[y_test == 1], bins=30, alpha=0.7, label='Readmission', density=True)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('DT: Probability Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Calibration plots\n",
        "plt.subplot(3, 3, 7)\n",
        "fraction_pos_lr, mean_pred_lr = calibration_curve(y_test, y_proba_lr, n_bins=10)\n",
        "plt.plot(mean_pred_lr, fraction_pos_lr, 'bo-', label='Logistic Regression')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Plot - LR')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(3, 3, 8)\n",
        "fraction_pos_dt, mean_pred_dt = calibration_curve(y_test, y_proba_dt, n_bins=10)\n",
        "plt.plot(mean_pred_dt, fraction_pos_dt, 'ro-', label='Decision Tree')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Plot - DT')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Performance metrics comparison\n",
        "plt.subplot(3, 3, 9)\n",
        "metrics_comparison = ['ROC_AUC', 'PR_AUC', 'Sensitivity', 'Specificity', 'F1']\n",
        "lr_values = [lr_metrics[metric] for metric in metrics_comparison]\n",
        "dt_values = [dt_metrics[metric] for metric in metrics_comparison]\n",
        "\n",
        "x = np.arange(len(metrics_comparison))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, lr_values, width, label='Logistic Regression', alpha=0.7)\n",
        "plt.bar(x + width/2, dt_values, width, label='Decision Tree', alpha=0.7)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics Comparison')\n",
        "plt.xticks(x, [m.replace('_', '\\n') for m in metrics_comparison])\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Error Analysis and Edge Cases\n",
        "\n",
        "### Understanding when and why models make incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== ERROR ANALYSIS ===\")\n",
        "\n",
        "# Create prediction analysis DataFrame\n",
        "X_test_analysis = X_test.copy()\n",
        "X_test_analysis['y_true'] = y_test\n",
        "X_test_analysis['y_pred_lr'] = y_pred_lr\n",
        "X_test_analysis['y_proba_lr'] = y_proba_lr\n",
        "X_test_analysis['y_pred_dt'] = y_pred_dt\n",
        "X_test_analysis['y_proba_dt'] = y_proba_dt\n",
        "\n",
        "# Identify different types of errors\n",
        "X_test_analysis['lr_error_type'] = 'Correct'\n",
        "X_test_analysis.loc[(X_test_analysis['y_true'] == 1) & (X_test_analysis['y_pred_lr'] == 0), 'lr_error_type'] = 'False Negative'\n",
        "X_test_analysis.loc[(X_test_analysis['y_true'] == 0) & (X_test_analysis['y_pred_lr'] == 1), 'lr_error_type'] = 'False Positive'\n",
        "\n",
        "X_test_analysis['dt_error_type'] = 'Correct'\n",
        "X_test_analysis.loc[(X_test_analysis['y_true'] == 1) & (X_test_analysis['y_pred_dt'] == 0), 'dt_error_type'] = 'False Negative'\n",
        "X_test_analysis.loc[(X_test_analysis['y_true'] == 0) & (X_test_analysis['y_pred_dt'] == 1), 'dt_error_type'] = 'False Positive'\n",
        "\n",
        "# Analyze error patterns\n",
        "print(\"\\nError Distribution:\")\n",
        "print(\"\\nLogistic Regression:\")\n",
        "lr_error_counts = X_test_analysis['lr_error_type'].value_counts()\n",
        "for error_type, count in lr_error_counts.items():\n",
        "    print(f\"  {error_type}: {count} ({count/len(X_test_analysis)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDecision Tree:\")\n",
        "dt_error_counts = X_test_analysis['dt_error_type'].value_counts()\n",
        "for error_type, count in dt_error_counts.items():\n",
        "    print(f\"  {error_type}: {count} ({count/len(X_test_analysis)*100:.1f}%)\")\n",
        "\n",
        "# Analyze characteristics of misclassified patients\n",
        "def analyze_error_characteristics(df, model_suffix):\n",
        "    \"\"\"\n",
        "    Analyze characteristics of misclassified patients\n",
        "    \"\"\"\n",
        "    error_col = f'{model_suffix}_error_type'\n",
        "    \n",
        "    print(f\"\\n=== {model_suffix.upper()} ERROR CHARACTERISTICS ===\")\n",
        "    \n",
        "    # False Negatives (missed readmissions)\n",
        "    false_negatives = df[df[error_col] == 'False Negative']\n",
        "    if len(false_negatives) > 0:\n",
        "        print(f\"\\nFalse Negatives (Missed Readmissions): {len(false_negatives)} cases\")\n",
        "        print(\"Average characteristics:\")\n",
        "        key_features = ['age', 'length_of_stay', 'previous_admissions', 'comorbidity_count']\n",
        "        for feature in key_features:\n",
        "            avg_fn = false_negatives[feature].mean()\n",
        "            avg_all = df[feature].mean()\n",
        "            print(f\"  {feature}: {avg_fn:.2f} (vs {avg_all:.2f} overall)\")\n",
        "        \n",
        "        print(\"  Medical conditions prevalence:\")\n",
        "        condition_features = ['has_diabetes', 'has_hypertension', 'has_heart_disease', 'has_kidney_disease']\n",
        "        for condition in condition_features:\n",
        "            fn_rate = false_negatives[condition].mean()\n",
        "            overall_rate = df[condition].mean()\n",
        "            print(f\"    {condition.replace('has_', '')}: {fn_rate*100:.1f}% (vs {overall_rate*100:.1f}% overall)\")\n",
        "    \n",
        "    # False Positives (incorrect high-risk predictions)\n",
        "    false_positives = df[df[error_col] == 'False Positive']\n",
        "    if len(false_positives) > 0:\n",
        "        print(f\"\\nFalse Positives (Incorrect High-Risk): {len(false_positives)} cases\")\n",
        "        print(\"Average characteristics:\")\n",
        "        for feature in key_features:\n",
        "            avg_fp = false_positives[feature].mean()\n",
        "            avg_all = df[feature].mean()\n",
        "            print(f\"  {feature}: {avg_fp:.2f} (vs {avg_all:.2f} overall)\")\n",
        "\n",
        "# Analyze errors for both models\n",
        "analyze_error_characteristics(X_test_analysis, 'lr')\n",
        "analyze_error_characteristics(X_test_analysis, 'dt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize error patterns\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Error distribution by key features\n",
        "key_features_analysis = ['age', 'length_of_stay', 'previous_admissions', 'comorbidity_count']\n",
        "\n",
        "for i, feature in enumerate(key_features_analysis):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    \n",
        "    # Box plots for each error type\n",
        "    error_types = ['Correct', 'False Negative', 'False Positive']\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "    \n",
        "    for error_type in error_types:\n",
        "        subset = X_test_analysis[X_test_analysis['lr_error_type'] == error_type][feature]\n",
        "        if len(subset) > 0:\n",
        "            data_to_plot.append(subset)\n",
        "            labels.append(f\"{error_type}\\n(n={len(subset)})\")\n",
        "    \n",
        "    if data_to_plot:\n",
        "        plt.boxplot(data_to_plot, labels=labels)\n",
        "        plt.title(f'LR Errors by {feature.replace(\"_\", \" \").title()}')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction confidence analysis\n",
        "plt.subplot(2, 4, 5)\n",
        "correct_predictions = X_test_analysis[X_test_analysis['lr_error_type'] == 'Correct']['y_proba_lr']\n",
        "false_negatives = X_test_analysis[X_test_analysis['lr_error_type'] == 'False Negative']['y_proba_lr']\n",
        "false_positives = X_test_analysis[X_test_analysis['lr_error_type'] == 'False Positive']['y_proba_lr']\n",
        "\n",
        "plt.hist(correct_predictions, bins=20, alpha=0.7, label='Correct', density=True)\n",
        "if len(false_negatives) > 0:\n",
        "    plt.hist(false_negatives, bins=20, alpha=0.7, label='False Negative', density=True)\n",
        "if len(false_positives) > 0:\n",
        "    plt.hist(false_positives, bins=20, alpha=0.7, label='False Positive', density=True)\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('LR: Prediction Confidence by Error Type')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Age group error analysis\n",
        "plt.subplot(2, 4, 6)\n",
        "age_groups = ['Under_40', '40_60', '60_80', 'Over_80']\n",
        "error_rates = []\n",
        "group_labels = []\n",
        "\n",
        "for age_group in age_groups:\n",
        "    age_col = f'age_group_{age_group}'\n",
        "    if age_col in X_test_analysis.columns:\n",
        "        group_patients = X_test_analysis[X_test_analysis[age_col] == 1]\n",
        "        if len(group_patients) > 0:\n",
        "            error_rate = (group_patients['lr_error_type'] != 'Correct').mean()\n",
        "            error_rates.append(error_rate * 100)\n",
        "            group_labels.append(f'{age_group.replace(\"_\", \"-\")}\\n(n={len(group_patients)})')\n",
        "\n",
        "if error_rates:\n",
        "    plt.bar(range(len(error_rates)), error_rates)\n",
        "    plt.xticks(range(len(error_rates)), group_labels)\n",
        "    plt.ylabel('Error Rate (%)')\n",
        "    plt.title('LR: Error Rate by Age Group')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# High-risk patient analysis\n",
        "plt.subplot(2, 4, 7)\n",
        "high_risk_correct = X_test_analysis[(X_test_analysis['high_risk_patient'] == 1) & \n",
        "                                   (X_test_analysis['lr_error_type'] == 'Correct')]\n",
        "high_risk_error = X_test_analysis[(X_test_analysis['high_risk_patient'] == 1) & \n",
        "                                 (X_test_analysis['lr_error_type'] != 'Correct')]\n",
        "standard_risk_correct = X_test_analysis[(X_test_analysis['high_risk_patient'] == 0) & \n",
        "                                       (X_test_analysis['lr_error_type'] == 'Correct')]\n",
        "standard_risk_error = X_test_analysis[(X_test_analysis['high_risk_patient'] == 0) & \n",
        "                                     (X_test_analysis['lr_error_type'] != 'Correct')]\n",
        "\n",
        "risk_categories = ['High Risk', 'Standard Risk']\n",
        "correct_counts = [len(high_risk_correct), len(standard_risk_correct)]\n",
        "error_counts = [len(high_risk_error), len(standard_risk_error)]\n",
        "\n",
        "x = np.arange(len(risk_categories))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, correct_counts, width, label='Correct', alpha=0.7)\n",
        "plt.bar(x + width/2, error_counts, width, label='Error', alpha=0.7)\n",
        "plt.xlabel('Risk Category')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.title('LR: Prediction Accuracy by Risk Category')\n",
        "plt.xticks(x, risk_categories)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model agreement analysis\n",
        "plt.subplot(2, 4, 8)\n",
        "agreement_matrix = pd.crosstab(X_test_analysis['y_pred_lr'], X_test_analysis['y_pred_dt'], \n",
        "                              rownames=['LR Prediction'], colnames=['DT Prediction'])\n",
        "sns.heatmap(agreement_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Model Agreement Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate model agreement statistics\n",
        "agreement_rate = (X_test_analysis['y_pred_lr'] == X_test_analysis['y_pred_dt']).mean()\n",
        "print(f\"\\n=== MODEL AGREEMENT ANALYSIS ===\")\n",
        "print(f\"Overall agreement rate: {agreement_rate*100:.1f}%\")\n",
        "\n",
        "# Cases where models disagree\n",
        "disagreement = X_test_analysis[X_test_analysis['y_pred_lr'] != X_test_analysis['y_pred_dt']]\n",
        "print(f\"Disagreement cases: {len(disagreement)} ({len(disagreement)/len(X_test_analysis)*100:.1f}%)\")\n",
        "\n",
        "if len(disagreement) > 0:\n",
        "    print(f\"\\nCharacteristics of disagreement cases:\")\n",
        "    print(f\"  Average age: {disagreement['age'].mean():.1f}\")\n",
        "    print(f\"  Average comorbidities: {disagreement['comorbidity_count'].mean():.1f}\")\n",
        "    print(f\"  High-risk patients: {disagreement['high_risk_patient'].mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clinical Scenario Testing\n",
        "\n",
        "### Testing model performance on specific clinical scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== CLINICAL SCENARIO TESTING ===\")\n",
        "\n",
        "def test_clinical_scenario(scenario_name, condition_func, X_test_df, y_test_series, y_proba_lr, y_proba_dt):\n",
        "    \"\"\"\n",
        "    Test model performance on specific clinical scenarios\n",
        "    \"\"\"\n",
        "    # Filter patients matching the scenario\n",
        "    scenario_mask = condition_func(X_test_df)\n",
        "    scenario_patients = X_test_df[scenario_mask]\n",
        "    scenario_y_true = y_test_series[scenario_mask]\n",
        "    scenario_y_proba_lr = y_proba_lr[scenario_mask]\n",
        "    scenario_y_proba_dt = y_proba_dt[scenario_mask]\n",
        "    \n",
        "    if len(scenario_patients) == 0:\n",
        "        print(f\"\\n{scenario_name}: No patients found matching criteria\")\n",
        "        return\n",
        "    \n",
        "    # Calculate metrics for this scenario\n",
        "    actual_readmission_rate = scenario_y_true.mean()\n",
        "    lr_auc = roc_auc_score(scenario_y_true, scenario_y_proba_lr) if len(scenario_y_true.unique()) > 1 else np.nan\n",
        "    dt_auc = roc_auc_score(scenario_y_true, scenario_y_proba_dt) if len(scenario_y_true.unique()) > 1 else np.nan\n",
        "    \n",
        "    print(f\"\\n{scenario_name}:\")\n",
        "    print(f\"  Patients: {len(scenario_patients)} ({len(scenario_patients)/len(X_test_df)*100:.1f}% of test set)\")\n",
        "    print(f\"  Actual readmission rate: {actual_readmission_rate*100:.1f}%\")\n",
        "    print(f\"  Average predicted risk (LR): {scenario_y_proba_lr.mean()*100:.1f}%\")\n",
        "    print(f\"  Average predicted risk (DT): {scenario_y_proba_dt.mean()*100:.1f}%\")\n",
        "    if not np.isnan(lr_auc):\n",
        "        print(f\"  Model performance - LR AUC: {lr_auc:.3f}, DT AUC: {dt_auc:.3f}\")\n",
        "    else:\n",
        "        print(f\"  Model performance: Cannot calculate AUC (insufficient variation in outcomes)\")\n",
        "    \n",
        "    return {\n",
        "        'scenario': scenario_name,\n",
        "        'n_patients': len(scenario_patients),\n",
        "        'readmission_rate': actual_readmission_rate,\n",
        "        'avg_risk_lr': scenario_y_proba_lr.mean(),\n",
        "        'avg_risk_dt': scenario_y_proba_dt.mean(),\n",
        "        'lr_auc': lr_auc,\n",
        "        'dt_auc': dt_auc\n",
        "    }\n",
        "\n",
        "# Define clinical scenarios\n",
        "scenarios = [\n",
        "    (\"Elderly Diabetic Patients\", \n",
        "     lambda df: (df['age'] >= 75) & (df['has_diabetes'] == 1)),\n",
        "    \n",
        "    (\"Young Patients (Under 50)\", \n",
        "     lambda df: df['age'] < 50),\n",
        "    \n",
        "    (\"Multiple Comorbidities (3+)\", \n",
        "     lambda df: df['comorbidity_count'] >= 3),\n",
        "    \n",
        "    (\"Emergency Admissions with Heart Disease\", \n",
        "     lambda df: (df['emergency_admission'] == 1) & (df['has_heart_disease'] == 1)),\n",
        "    \n",
        "    (\"Recent Frequent Readmitters\", \n",
        "     lambda df: df['frequent_readmitter'] == 1),\n",
        "    \n",
        "    (\"Long Stay Patients (>10 days)\", \n",
        "     lambda df: df['length_of_stay'] > 10),\n",
        "    \n",
        "    (\"First-Time Admissions\", \n",
        "     lambda df: df['previous_admissions'] == 0),\n",
        "    \n",
        "    (\"Medicare Patients with Kidney Disease\", \n",
        "     lambda df: (df['insurance_std_Medicare'] == 1) & (df['has_kidney_disease'] == 1)),\n",
        "    \n",
        "    (\"High-Risk Elderly Emergency Patients\", \n",
        "     lambda df: (df['age'] >= 70) & (df['emergency_admission'] == 1) & (df['comorbidity_count'] >= 2))\n",
        "]\n",
        "\n",
        "# Test all scenarios\n",
        "scenario_results = []\n",
        "for scenario_name, condition_func in scenarios:\n",
        "    result = test_clinical_scenario(scenario_name, condition_func, X_test, y_test, y_proba_lr, y_proba_dt)\n",
        "    if result:\n",
        "        scenario_results.append(result)\n",
        "\n",
        "# Create scenario summary DataFrame\n",
        "if scenario_results:\n",
        "    scenario_df = pd.DataFrame(scenario_results)\n",
        "    \n",
        "    print(f\"\\n=== CLINICAL SCENARIO SUMMARY ===\")\n",
        "    summary_df = scenario_df[['scenario', 'n_patients', 'readmission_rate', 'avg_risk_lr', 'lr_auc']].copy()\n",
        "    summary_df['readmission_rate'] = (summary_df['readmission_rate'] * 100).round(1)\n",
        "    summary_df['avg_risk_lr'] = (summary_df['avg_risk_lr'] * 100).round(1)\n",
        "    summary_df['lr_auc'] = summary_df['lr_auc'].round(3)\n",
        "    summary_df.columns = ['Scenario', 'N Patients', 'Actual Rate (%)', 'Predicted Risk (%)', 'LR AUC']\n",
        "    \n",
        "    print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Bias Detection and Fairness Assessment\n",
        "\n",
        "### Ensuring equitable performance across patient demographics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== BIAS DETECTION AND FAIRNESS ASSESSMENT ===\")\n",
        "\n",
        "def assess_fairness_by_group(group_col, group_name, X_test_df, y_test_series, y_pred_lr, y_proba_lr):\n",
        "    \"\"\"\n",
        "    Assess model fairness across different demographic groups\n",
        "    \"\"\"\n",
        "    print(f\"\\n{group_name} Fairness Analysis:\")\n",
        "    \n",
        "    fairness_results = []\n",
        "    \n",
        "    # Get unique groups\n",
        "    if group_col.startswith('age_group_'):\n",
        "        # Handle age groups (one-hot encoded)\n",
        "        age_groups = ['Under_40', '40_60', '60_80', 'Over_80']\n",
        "        for age_group in age_groups:\n",
        "            col_name = f'age_group_{age_group}'\n",
        "            if col_name in X_test_df.columns:\n",
        "                group_mask = X_test_df[col_name] == 1\n",
        "                if group_mask.sum() > 0:\n",
        "                    fairness_results.append(assess_group_performance(\n",
        "                        age_group.replace('_', '-'), group_mask, y_test_series, y_pred_lr, y_proba_lr\n",
        "                    ))\n",
        "    elif group_col.startswith('gender_std_'):\n",
        "        # Handle gender groups\n",
        "        genders = ['Female', 'Male']\n",
        "        for gender in genders:\n",
        "            col_name = f'gender_std_{gender}'\n",
        "            if col_name in X_test_df.columns:\n",
        "                group_mask = X_test_df[col_name] == 1\n",
        "                if group_mask.sum() > 0:\n",
        "                    fairness_results.append(assess_group_performance(\n",
        "                        gender, group_mask, y_test_series, y_pred_lr, y_proba_lr\n",
        "                    ))\n",
        "    elif group_col.startswith('insurance_std_'):\n",
        "        # Handle insurance groups\n",
        "        insurance_types = ['Medicare', 'Private', 'Medicaid', 'Other']\n",
        "        for insurance in insurance_types:\n",
        "            col_name = f'insurance_std_{insurance}'\n",
        "            if col_name in X_test_df.columns:\n",
        "                group_mask = X_test_df[col_name] == 1\n",
        "                if group_mask.sum() > 0:\n",
        "                    fairness_results.append(assess_group_performance(\n",
        "                        insurance, group_mask, y_test_series, y_pred_lr, y_proba_lr\n",
        "                    ))\n",
        "    \n",
        "    if fairness_results:\n",
        "        fairness_df = pd.DataFrame(fairness_results)\n",
        "        print(fairness_df.to_string(index=False))\n",
        "        \n",
        "        # Check for significant disparities\n",
        "        if len(fairness_df) > 1:\n",
        "            max_auc = fairness_df['AUC'].max()\n",
        "            min_auc = fairness_df['AUC'].min()\n",
        "            auc_disparity = max_auc - min_auc\n",
        "            \n",
        "            max_fpr = fairness_df['FPR'].max()\n",
        "            min_fpr = fairness_df['FPR'].min()\n",
        "            fpr_disparity = max_fpr - min_fpr\n",
        "            \n",
        "            print(f\"\\n  Disparity Analysis:\")\n",
        "            print(f\"    AUC disparity: {auc_disparity:.3f} (max: {max_auc:.3f}, min: {min_auc:.3f})\")\n",
        "            print(f\"    FPR disparity: {fpr_disparity:.3f} (max: {max_fpr:.3f}, min: {min_fpr:.3f})\")\n",
        "            \n",
        "            if auc_disparity > 0.05:\n",
        "                print(f\"    ⚠️  Significant AUC disparity detected (>{0.05:.3f})\")\n",
        "            if fpr_disparity > 0.05:\n",
        "                print(f\"    ⚠️  Significant FPR disparity detected (>{0.05:.3f})\")\n",
        "        \n",
        "        return fairness_df\n",
        "    \n",
        "    return None\n",
        "\n",
        "def assess_group_performance(group_name, group_mask, y_true, y_pred, y_proba):\n",
        "    \"\"\"\n",
        "    Calculate performance metrics for a specific group\n",
        "    \"\"\"\n",
        "    group_y_true = y_true[group_mask]\n",
        "    group_y_pred = y_pred[group_mask]\n",
        "    group_y_proba = y_proba[group_mask]\n",
        "    \n",
        "    if len(group_y_true) == 0:\n",
        "        return None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    n_patients = len(group_y_true)\n",
        "    prevalence = group_y_true.mean()\n",
        "    \n",
        "    if len(group_y_true.unique()) > 1:\n",
        "        auc = roc_auc_score(group_y_true, group_y_proba)\n",
        "        precision = precision_score(group_y_true, group_y_pred, zero_division=0)\n",
        "        recall = recall_score(group_y_true, group_y_pred, zero_division=0)\n",
        "        \n",
        "        # Calculate FPR\n",
        "        tn, fp, fn, tp = confusion_matrix(group_y_true, group_y_pred).ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    else:\n",
        "        auc = np.nan\n",
        "        precision = np.nan\n",
        "        recall = np.nan\n",
        "        fpr = np.nan\n",
        "    \n",
        "    return {\n",
        "        'Group': group_name,\n",
        "        'N': n_patients,\n",
        "        'Prevalence': prevalence,\n",
        "        'AUC': auc,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'FPR': fpr\n",
        "    }\n",
        "\n",
        "# Assess fairness across different demographic groups\n",
        "demographic_groups = [\n",
        "    ('age_group_', 'Age Groups'),\n",
        "    ('gender_std_', 'Gender'),\n",
        "    ('insurance_std_', 'Insurance Type')\n",
        "]\n",
        "\n",
        "all_fairness_results = {}\n",
        "for group_prefix, group_name in demographic_groups:\n",
        "    fairness_df = assess_fairness_by_group(group_prefix, group_name, X_test, y_test, y_pred_lr, y_proba_lr)\n",
        "    if fairness_df is not None:\n",
        "        all_fairness_results[group_name] = fairness_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize fairness assessment results\n",
        "if all_fairness_results:\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    plot_idx = 1\n",
        "    for group_name, fairness_df in all_fairness_results.items():\n",
        "        if len(fairness_df) > 1:  # Only plot if multiple groups\n",
        "            # AUC comparison\n",
        "            plt.subplot(2, 3, plot_idx)\n",
        "            valid_auc = fairness_df.dropna(subset=['AUC'])\n",
        "            if len(valid_auc) > 0:\n",
        "                bars = plt.bar(valid_auc['Group'], valid_auc['AUC'])\n",
        "                plt.ylabel('AUC Score')\n",
        "                plt.title(f'{group_name}: AUC by Group')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.ylim(0, 1)\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                # Add value labels on bars\n",
        "                for bar, value in zip(bars, valid_auc['AUC']):\n",
        "                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                            f'{value:.3f}', ha='center', va='bottom')\n",
        "            \n",
        "            # False Positive Rate comparison\n",
        "            plt.subplot(2, 3, plot_idx + 3)\n",
        "            valid_fpr = fairness_df.dropna(subset=['FPR'])\n",
        "            if len(valid_fpr) > 0:\n",
        "                bars = plt.bar(valid_fpr['Group'], valid_fpr['FPR'])\n",
        "                plt.ylabel('False Positive Rate')\n",
        "                plt.title(f'{group_name}: FPR by Group')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.ylim(0, max(valid_fpr['FPR']) * 1.1)\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                \n",
        "                # Add value labels on bars\n",
        "                for bar, value in zip(bars, valid_fpr['FPR']):\n",
        "                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                            f'{value:.3f}', ha='center', va='bottom')\n",
        "            \n",
        "            plot_idx += 1\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Calibration Analysis\n",
        "\n",
        "### Ensuring predicted probabilities match actual outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== MODEL CALIBRATION ANALYSIS ===\")\n",
        "\n",
        "# Calibration analysis\n",
        "def analyze_calibration(y_true, y_proba, model_name, n_bins=10):\n",
        "    \"\"\"\n",
        "    Analyze model calibration and provide detailed assessment\n",
        "    \"\"\"\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_true, y_proba, n_bins=n_bins\n",
        "    )\n",
        "    \n",
        "    # Calculate calibration metrics\n",
        "    brier_score = brier_score_loss(y_true, y_proba)\n",
        "    \n",
        "    # Calculate Expected Calibration Error (ECE)\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "    \n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (y_proba > bin_lower) & (y_proba <= bin_upper)\n",
        "        prop_in_bin = in_bin.mean()\n",
        "        \n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = y_true[in_bin].mean()\n",
        "            avg_confidence_in_bin = y_proba[in_bin].mean()\n",
        "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "    \n",
        "    print(f\"\\\\n{model_name} Calibration Analysis:\")\n",
        "    print(f\"  Brier Score: {brier_score:.4f} (lower is better)\")\n",
        "    print(f\"  Expected Calibration Error (ECE): {ece:.4f} (lower is better)\")\n",
        "    \n",
        "    # Interpretation\n",
        "    if ece < 0.05:\n",
        "        calibration_quality = \"Excellent\"\n",
        "    elif ece < 0.10:\n",
        "        calibration_quality = \"Good\"\n",
        "    elif ece < 0.15:\n",
        "        calibration_quality = \"Fair\"\n",
        "    else:\n",
        "        calibration_quality = \"Poor\"\n",
        "    \n",
        "    print(f\"  Calibration Quality: {calibration_quality}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'brier_score': brier_score,\n",
        "        'ece': ece,\n",
        "        'calibration_quality': calibration_quality,\n",
        "        'fraction_of_positives': fraction_of_positives,\n",
        "        'mean_predicted_value': mean_predicted_value\n",
        "    }\n",
        "\n",
        "# Analyze calibration for both models\n",
        "lr_calibration = analyze_calibration(y_test, y_proba_lr, \"Logistic Regression\")\n",
        "dt_calibration = analyze_calibration(y_test, y_proba_dt, \"Decision Tree\")\n",
        "\n",
        "# Detailed calibration visualization\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Calibration plots with reliability diagrams\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(lr_calibration['mean_predicted_value'], lr_calibration['fraction_of_positives'], \n",
        "         'bo-', markersize=8, label='Logistic Regression')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title(f'LR Calibration Plot\\\\n(ECE: {lr_calibration[\"ece\"]:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(dt_calibration['mean_predicted_value'], dt_calibration['fraction_of_positives'], \n",
        "         'ro-', markersize=8, label='Decision Tree')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title(f'DT Calibration Plot\\\\n(ECE: {dt_calibration[\"ece\"]:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Calibration comparison\n",
        "plt.subplot(2, 3, 3)\n",
        "models = ['Logistic Regression', 'Decision Tree']\n",
        "brier_scores = [lr_calibration['brier_score'], dt_calibration['brier_score']]\n",
        "ece_scores = [lr_calibration['ece'], dt_calibration['ece']]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, brier_scores, width, label='Brier Score', alpha=0.7)\n",
        "plt.bar(x + width/2, ece_scores, width, label='ECE', alpha=0.7)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Score (Lower is Better)')\n",
        "plt.title('Calibration Metrics Comparison')\n",
        "plt.xticks(x, models)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction distribution by bins\n",
        "plt.subplot(2, 3, 4)\n",
        "bin_edges = np.linspace(0, 1, 11)\n",
        "plt.hist(y_proba_lr, bins=bin_edges, alpha=0.7, label='Logistic Regression', density=True)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('LR: Prediction Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.hist(y_proba_dt, bins=bin_edges, alpha=0.7, label='Decision Tree', density=True, color='red')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('DT: Prediction Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Risk stratification analysis\n",
        "plt.subplot(2, 3, 6)\n",
        "risk_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "lr_ppv_at_threshold = []\n",
        "dt_ppv_at_threshold = []\n",
        "\n",
        "for threshold in risk_thresholds:\n",
        "    lr_high_risk = y_proba_lr >= threshold\n",
        "    dt_high_risk = y_proba_dt >= threshold\n",
        "    \n",
        "    lr_ppv = y_test[lr_high_risk].mean() if lr_high_risk.sum() > 0 else 0\n",
        "    dt_ppv = y_test[dt_high_risk].mean() if dt_high_risk.sum() > 0 else 0\n",
        "    \n",
        "    lr_ppv_at_threshold.append(lr_ppv)\n",
        "    dt_ppv_at_threshold.append(dt_ppv)\n",
        "\n",
        "plt.plot(risk_thresholds, lr_ppv_at_threshold, 'bo-', label='Logistic Regression')\n",
        "plt.plot(risk_thresholds, dt_ppv_at_threshold, 'ro-', label='Decision Tree')\n",
        "plt.xlabel('Risk Threshold')\n",
        "plt.ylabel('Positive Predictive Value')\n",
        "plt.title('PPV at Different Risk Thresholds')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\n=== CALIBRATION RECOMMENDATIONS ===\")\n",
        "print(f\"✓ Logistic Regression shows {lr_calibration['calibration_quality'].lower()} calibration\")\n",
        "print(f\"✓ Decision Tree shows {dt_calibration['calibration_quality'].lower()} calibration\")\n",
        "\n",
        "if lr_calibration['ece'] < dt_calibration['ece']:\n",
        "    print(f\"✓ Logistic Regression is better calibrated (ECE: {lr_calibration['ece']:.4f} vs {dt_calibration['ece']:.4f})\")\n",
        "    print(f\"✓ LR probabilities can be trusted for clinical decision-making\")\n",
        "else:\n",
        "    print(f\"✓ Decision Tree is better calibrated (ECE: {dt_calibration['ece']:.4f} vs {lr_calibration['ece']:.4f})\")\n",
        "\n",
        "if max(lr_calibration['ece'], dt_calibration['ece']) > 0.10:\n",
        "    print(f\"⚠️  Consider calibration techniques (Platt scaling, isotonic regression) for deployment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Model Evaluation Summary\n",
        "\n",
        "### Comprehensive assessment and clinical recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== FINAL MODEL EVALUATION SUMMARY ===\")\n",
        "print(f\"\\\\nProject: Hospital Readmission Risk Prediction\")\n",
        "print(f\"Evaluation Phase Complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"\\\\nInternship Context: Southeast Texas Regional Hospitals, 2015\")\n",
        "\n",
        "print(f\"\\\\n=== DATASET SUMMARY ===\")\n",
        "print(f\"Test set evaluation: {len(X_test)} patients\")\n",
        "print(f\"Actual readmission rate: {y_test.mean()*100:.1f}%\")\n",
        "print(f\"Features evaluated: {X_test.shape[1]}\")\n",
        "\n",
        "print(f\"\\\\n=== MODEL PERFORMANCE COMPARISON ===\")\n",
        "comparison_metrics = {\n",
        "    'Metric': ['ROC-AUC', 'Precision', 'Recall', 'F1-Score', 'Brier Score', 'ECE'],\n",
        "    'Logistic Regression': [\n",
        "        f\"{lr_metrics['ROC_AUC']:.3f}\",\n",
        "        f\"{lr_metrics['Precision']:.3f}\",\n",
        "        f\"{lr_metrics['Recall']:.3f}\",\n",
        "        f\"{lr_metrics['F1']:.3f}\",\n",
        "        f\"{lr_calibration['brier_score']:.4f}\",\n",
        "        f\"{lr_calibration['ece']:.4f}\"\n",
        "    ],\n",
        "    'Decision Tree': [\n",
        "        f\"{dt_metrics['ROC_AUC']:.3f}\",\n",
        "        f\"{dt_metrics['Precision']:.3f}\",\n",
        "        f\"{dt_metrics['Recall']:.3f}\",\n",
        "        f\"{dt_metrics['F1']:.3f}\",\n",
        "        f\"{dt_calibration['brier_score']:.4f}\",\n",
        "        f\"{dt_calibration['ece']:.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_metrics)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\\\n=== CLINICAL PERFORMANCE ASSESSMENT ===\")\n",
        "print(f\"\\\\nLogistic Regression:\")\n",
        "print(f\"  • Identifies {lr_metrics['Sensitivity']*100:.1f}% of actual readmissions\")\n",
        "print(f\"  • {lr_metrics['PPV']*100:.1f}% of high-risk predictions are correct\")\n",
        "print(f\"  • Requires screening {lr_metrics['NNS']:.1f} patients per true readmission\")\n",
        "print(f\"  • {lr_calibration['calibration_quality']} probability calibration\")\n",
        "\n",
        "print(f\"\\\\nDecision Tree:\")\n",
        "print(f\"  • Identifies {dt_metrics['Sensitivity']*100:.1f}% of actual readmissions\")\n",
        "print(f\"  • {dt_metrics['PPV']*100:.1f}% of high-risk predictions are correct\")\n",
        "print(f\"  • Requires screening {dt_metrics['NNS']:.1f} patients per true readmission\")\n",
        "print(f\"  • {dt_calibration['calibration_quality']} probability calibration\")\n",
        "\n",
        "print(f\"\\\\n=== ERROR ANALYSIS INSIGHTS ===\")\n",
        "lr_fn_rate = (X_test_analysis['lr_error_type'] == 'False Negative').mean()\n",
        "lr_fp_rate = (X_test_analysis['lr_error_type'] == 'False Positive').mean()\n",
        "print(f\"\\\\nMissed readmissions (False Negatives): {lr_fn_rate*100:.1f}%\")\n",
        "print(f\"Incorrect high-risk predictions (False Positives): {lr_fp_rate*100:.1f}%\")\n",
        "print(f\"Model agreement rate: {agreement_rate*100:.1f}%\")\n",
        "\n",
        "print(f\"\\\\n=== FAIRNESS ASSESSMENT ===\")\n",
        "fairness_issues = []\n",
        "for group_name, fairness_df in all_fairness_results.items():\n",
        "    if len(fairness_df) > 1:\n",
        "        auc_disparity = fairness_df['AUC'].max() - fairness_df['AUC'].min()\n",
        "        if auc_disparity > 0.05:\n",
        "            fairness_issues.append(f\"{group_name}: AUC disparity of {auc_disparity:.3f}\")\n",
        "\n",
        "if fairness_issues:\n",
        "    print(f\"⚠️  Fairness concerns identified:\")\n",
        "    for issue in fairness_issues:\n",
        "        print(f\"    {issue}\")\n",
        "else:\n",
        "    print(f\"✓ No significant fairness disparities detected across demographic groups\")\n",
        "\n",
        "print(f\"\\\\n=== CLINICAL SCENARIO PERFORMANCE ===\")\n",
        "if scenario_results:\n",
        "    high_risk_scenarios = [r for r in scenario_results if r['readmission_rate'] > 0.3]\n",
        "    if high_risk_scenarios:\n",
        "        print(f\"High-risk scenarios identified:\")\n",
        "        for scenario in high_risk_scenarios:\n",
        "            print(f\"  • {scenario['scenario']}: {scenario['readmission_rate']*100:.1f}% actual rate\")\n",
        "    \n",
        "    challenging_scenarios = [r for r in scenario_results if not np.isnan(r['lr_auc']) and r['lr_auc'] < 0.7]\n",
        "    if challenging_scenarios:\n",
        "        print(f\"\\\\nChallenging scenarios (AUC < 0.7):\")\n",
        "        for scenario in challenging_scenarios:\n",
        "            print(f\"  • {scenario['scenario']}: AUC = {scenario['lr_auc']:.3f}\")\n",
        "\n",
        "print(f\"\\\\n=== FINAL RECOMMENDATIONS ===\")\n",
        "\n",
        "# Choose recommended model based on overall performance\n",
        "if lr_metrics['ROC_AUC'] >= dt_metrics['ROC_AUC'] and lr_calibration['ece'] <= dt_calibration['ece']:\n",
        "    recommended_model = \"Logistic Regression\"\n",
        "    recommended_metrics = lr_metrics\n",
        "    recommended_calibration = lr_calibration\n",
        "else:\n",
        "    recommended_model = \"Decision Tree\"\n",
        "    recommended_metrics = dt_metrics\n",
        "    recommended_calibration = dt_calibration\n",
        "\n",
        "print(f\"\\\\n🎯 RECOMMENDED MODEL: {recommended_model}\")\n",
        "print(f\"\\\\nRationale:\")\n",
        "if recommended_model == \"Logistic Regression\":\n",
        "    print(f\"✓ Superior discrimination (AUC: {recommended_metrics['ROC_AUC']:.3f})\")\n",
        "    print(f\"✓ Better probability calibration (ECE: {recommended_calibration['ece']:.4f})\")\n",
        "    print(f\"✓ Interpretable coefficients for clinical staff\")\n",
        "    print(f\"✓ Robust performance across patient populations\")\n",
        "    print(f\"✓ Suitable for continuous risk scoring\")\n",
        "else:\n",
        "    print(f\"✓ Rule-based predictions easy to follow\")\n",
        "    print(f\"✓ No scaling required for implementation\")\n",
        "    print(f\"✓ Can be converted to clinical decision trees\")\n",
        "    print(f\"✓ Good performance (AUC: {recommended_metrics['ROC_AUC']:.3f})\")\n",
        "\n",
        "print(f\"\\\\n=== DEPLOYMENT CONSIDERATIONS ===\")\n",
        "print(f\"✓ Model ready for clinical integration\")\n",
        "print(f\"✓ Performance validated across demographic groups\")\n",
        "print(f\"✓ Error patterns understood and documented\")\n",
        "print(f\"✓ Calibration quality assessed: {recommended_calibration['calibration_quality']}\")\n",
        "\n",
        "if recommended_calibration['ece'] > 0.10:\n",
        "    print(f\"⚠️  Recommend calibration improvement before deployment\")\n",
        "\n",
        "print(f\"\\\\n=== NEXT STEPS ===\")\n",
        "print(f\"1. Dashboard Development (05_dashboard.ipynb):\")\n",
        "print(f\"   - Interactive risk assessment interface\")\n",
        "print(f\"   - Real-time prediction capabilities\")\n",
        "print(f\"   - Clinical decision support features\")\n",
        "print(f\"\\\\n2. Clinical Validation:\")\n",
        "print(f\"   - Pilot testing with clinical staff\")\n",
        "print(f\"   - User experience feedback\")\n",
        "print(f\"   - Integration with hospital workflows\")\n",
        "\n",
        "print(f\"\\\\n=== INTERNSHIP IMPACT ===\")\n",
        "print(f\"🏥 Successfully developed interpretable readmission risk models\")\n",
        "print(f\"📊 Achieved clinically relevant performance metrics\")\n",
        "print(f\"🤝 Collaborated effectively with clinical domain experts\")\n",
        "print(f\"⚖️ Ensured fairness across patient populations\")\n",
        "print(f\"🔍 Provided comprehensive model evaluation and validation\")\n",
        "\n",
        "print(f\"\\\\nThis evaluation demonstrates the successful completion of a challenging\")\n",
        "print(f\"healthcare data science project, showcasing skills in model development,\")\n",
        "print(f\"clinical validation, and ethical AI considerations.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
